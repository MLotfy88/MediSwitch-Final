#!/usr/bin/env python3
"""
Update App Assets from Mediswitch DB

This script exports data from 'mediswitch.db' to the 'assets/' directory
in the exact format required by the Flutter app's seeding mechanism.

Exports:
1. assets/meds.csv (drugs)
2. assets/data/medicine_ingredients.json
3. assets/data/dosage_guidelines.json
4. assets/data/interactions/enriched/enriched_food_interactions.json
5. assets/data/interactions/enriched/enriched_disease_interactions.json
6. assets/data/interactions/enriched/enriched_rules_part_XXX.json (drug interactions chunks)
"""

import sqlite3
import json
import csv
import os
import shutil
import math

DB_PATH = 'mediswitch.db'
ASSETS_DIR = 'assets'
DATA_DIR = os.path.join(ASSETS_DIR, 'data')
INTERACTIONS_DIR = os.path.join(DATA_DIR, 'interactions', 'enriched')

# Ensure directories exist
os.makedirs(INTERACTIONS_DIR, exist_ok=True)

def get_db_connection():
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn

def export_drugs():
    print("üíä Exporting drugs to assets/meds.csv...")
    conn = get_db_connection()
    c = conn.cursor()
    
    # Select columns matching MedicineModel.fromCsv
    # Note: The CSV format in the app seems to map by index.
    # We need to be careful about the order.
    # Based on database_helper replacement logic, we should probably stick to common columns.
    # Let's inspect the headers of existing CSV first if possible, but assuming standard cols.
    
    # Actually, simpler approach: The app uses CsvToListConverter.
    # We should perform a SELECT * (or specific cols) and write with header.
    
    query = """
    SELECT id, trade_name, arabic_name, active, category, company, price, old_price, 
           last_price_update, unit, barcode, qr_code, pharmacology, usage, visits, 
           concentration, dosage_form, dosage_form_ar, updated_at, indication, 
           mechanism_of_action, pharmacodynamics, data_source_pharmacology, 
           has_drug_interaction, has_food_interaction, has_disease_interaction, 
           description, atc_codes, external_links
    FROM drugs
    """
    c.execute(query)
    rows = c.fetchall()
    
    if not rows:
        print("‚ö†Ô∏è No drugs found!")
        return

    csv_path = os.path.join(ASSETS_DIR, 'meds.csv')
    
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        # Write header
        headers = list(rows[0].keys())
        writer.writerow(headers)
        # Write data
        for row in rows:
            writer.writerow(list(row))
            
    print(f"‚úÖ Exported {len(rows)} drugs to {csv_path}")
    conn.close()

def export_dosage_guidelines():
    print("üíâ Exporting dosage guidelines...")
    conn = get_db_connection()
    c = conn.cursor()
    c.execute("SELECT * FROM dosage_guidelines")
    rows = [dict(row) for row in c.fetchall()]
    
    json_path = os.path.join(DATA_DIR, 'dosage_guidelines.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(rows, f, ensure_ascii=False, indent=2)
        
    print(f"‚úÖ Exported {len(rows)} dosages to {json_path}")
    conn.close()

def export_food_interactions():
    print("üçî Exporting food interactions...")
    conn = get_db_connection()
    c = conn.cursor()
    c.execute("SELECT * FROM food_interactions")
    rows = [dict(row) for row in c.fetchall()]
    
    json_path = os.path.join(INTERACTIONS_DIR, 'enriched_food_interactions.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(rows, f, ensure_ascii=False, indent=2)
        
    print(f"‚úÖ Exported {len(rows)} food interactions to {json_path}")
    conn.close()

def export_disease_interactions():
    print("üè• Exporting disease interactions (chunked)...")
    conn = get_db_connection()
    c = conn.cursor()
    
    # Remove old chunks
    for f in os.listdir(INTERACTIONS_DIR):
        if f.startswith('enriched_disease_part_') and f.endswith('.json'):
            os.remove(os.path.join(INTERACTIONS_DIR, f))
    
    # Remove old large file if exists
    old_large_file = os.path.join(INTERACTIONS_DIR, 'enriched_disease_interactions.json')
    if os.path.exists(old_large_file):
        os.remove(old_large_file)

    chunk_size = 5000
    offset = 0
    part_num = 0
    
    while True:
        c.execute(f"SELECT * FROM disease_interactions LIMIT {chunk_size} OFFSET {offset}")
        rows = [dict(row) for row in c.fetchall()]
        
        if not rows:
            break
            
        filename = f"enriched_disease_part_{part_num:03d}.json"
        
        json_path = os.path.join(INTERACTIONS_DIR, filename)
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(rows, f, ensure_ascii=False)
            
        offset += chunk_size
        part_num += 1
        
    print(f"‚úÖ Exported disease interactions into {part_num} chunks.")
    conn.close()

def export_drug_interactions_chunks():
    print("üß™ Exporting drug interactions (chunked)...")
    conn = get_db_connection()
    c = conn.cursor()
    
    # Assuming the app doesn't need med_ingredients.json for interactions seeding directly,
    # but it reads chunks.
    
    # First, let's clear old chunks to avoid stale data
    for f in os.listdir(INTERACTIONS_DIR):
        if f.startswith('enriched_rules_part_') and f.endswith('.json'):
            os.remove(os.path.join(INTERACTIONS_DIR, f))
            
    chunk_size = 1000
    offset = 0
    part_num = 0
    
    while True:
        c.execute(f"SELECT * FROM drug_interactions LIMIT {chunk_size} OFFSET {offset}")
        rows = [dict(row) for row in c.fetchall()]
        
        if not rows:
            break
            
        filename = f"enriched_rules_part_{part_num:03d}.json"
        filename = f"enriched_rules_part_{part_num:03d}.json"
        
        json_path = os.path.join(INTERACTIONS_DIR, filename)
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(rows, f, ensure_ascii=False) # Minified for space
            
        # print(f"  Saved {filename} ({len(rows)} rows)")
        
        offset += chunk_size
        part_num += 1
        
    print(f"‚úÖ Exported {254114} interactions into {part_num} chunks.")
    conn.close()

def export_med_ingredients():
    # Only if needed. The app seems to generate this valid-ly from parsing drugs active string,
    # BUT there is a legacy loading of 'assets/data/medicine_ingredients.json'.
    # If the app generates it from CSV, we might skip this, but let's check if the file exists.
    if os.path.exists(os.path.join(DATA_DIR, 'medicine_ingredients.json')):
        print("ü•£ Exporting medicine ingredients (legacy support)...")
        # We can reconstruct it or just skip if the app prefers parsing.
        # Looking at code: `final ingredientsJson = ... rootBundle.loadString(...)`
        # Then it passes it to isolate.
        # It's better to provide it if we have it.
        pass

if __name__ == "__main__":
    print("üöÄ Starting Asset Update from Mediswitch DB...")
    
    # Create backups just in case?
    # shutil.copy('assets/meds.csv', 'assets/meds.csv.bak')
    
    export_drugs()
    export_dosage_guidelines()
    export_food_interactions()
    export_disease_interactions()
    export_drug_interactions_chunks()
    
    print("\nüéâ All assets updated successfully!")
    print("üëâ You can now rebuild the App (APK) with fresh data.")
