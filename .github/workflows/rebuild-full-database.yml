name: Rebuild Full Database (Unified)
# STRICT ORDER IMPLEMENTATION:
# 1. Scrape DwaPrices -> Update meds.csv -> Commit
# 2. DailyMed Cache Check (Parts) -> IF FOUND: Jump to Step 8
# 3. Download DailyMed -> Extract to Unified File
# 4. Split Unified File -> 4 Parts -> Commit (Keep Full for Working)
# 5. Extract Interactions (Reads Zips) -> Commit
# 6. Extract Dosages (Reads Unified File) -> Commit
# 7. Merge Hybrid -> Bootstrap -> Sync D1

on:
  workflow_dispatch:
    inputs:
      force_scrape:
        description: 'Force Full Re-scrape?'
        required: false
        default: 'false'
        type: boolean
      test_mode:
        description: 'Test mode limit (0=all)'
        required: false
        default: '0'
        type: string

jobs:
  full-rebuild:
    runs-on: ubuntu-latest
    timeout-minutes: 360 # 6 Hours Max
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          
      - name: Install Dependencies
        run: |
          pip install requests pandas beautifulsoup4 aiohttp aiofiles brotli

      # --- STEP 1: Scrape & Commit ---
      - name: 1. Scrape DwaPrices & Update meds.csv
        continue-on-error: true
        run: |
          echo "::group::Scraping DwaPrices"
          LIMIT_FLAG=""
          if [ "${{ inputs.test_mode }}" != "0" ] && [ -n "${{ inputs.test_mode }}" ]; then
            LIMIT_FLAG="--limit ${{ inputs.test_mode }}"
          fi
          
          if [ "${{ inputs.force_scrape }}" == "true" ]; then
            python3 -u scripts/scrape_dwaprices_by_id.py --reset $LIMIT_FLAG
          else
            python3 -u scripts/scrape_dwaprices_by_id.py $LIMIT_FLAG
          fi
          
          # Update CSV from JSONL
          python3 scripts/update_meds.py
          echo "::endgroup::"

      - name: 1.1 Generate Known Ingredients (Required for Interactions)
        run: |
          # New Step: Generate known_ingredients.json from scrape data
          python3 scripts/generate_ingredients.py

      - name: 1.2 Commit Scraped Data
        if: always()
        run: |
          git config --global user.name 'DailyMed Bot'
          git config --global user.email 'bot@dailymed.local'
          
          git add -f assets/meds.csv
          git add -f assets/meds_scraped_new.jsonl
          git add -f production_data/known_ingredients.json
          
          git commit -m "data: scrape update $(date +'%Y-%m-%d')" || echo "No changes"
          git push --force-with-lease || git push

      # --- STEP 2: Cache Check (Search for Parts) ---
      - name: 2. Search for DailyMed Cache (Parts)
        id: check_cache
        run: |
          # User requested looking for split parts
          PARTS_DIR="production_data/dailymed_full"
          TARGET_FILE="production_data/dailymed_full_database.jsonl.gz"
          
          # Check if we have parts in the repo (or restored from cache)
          # Assuming we are looking at the committed parts in repo
          if [ -d "$PARTS_DIR" ] && [ "$(ls -A $PARTS_DIR)" ]; then
             echo "âœ… Found valid cached DailyMed Parts in $PARTS_DIR"
             
             # Reassemble for later steps (Dosages needs this)
             echo "ðŸ§© Reassembling Unified Database..."
             python3 scripts/manage_large_file.py join "$PARTS_DIR" "$TARGET_FILE"
             
             echo "skip_dailymed_download=true" >> $GITHUB_OUTPUT
          else
             echo "âŒ No Valid Cache Found using parts."
             echo "skip_dailymed_download=false" >> $GITHUB_OUTPUT
          fi

      # --- STEP 3: Download & Extract (If No Cache) ---
      - name: 3. Download DailyMed Full DB
        if: steps.check_cache.outputs.skip_dailymed_download != 'true'
        run: |
          python3 scripts/download_dailymed.py

      - name: 3.1 Extract to Unified File
        if: steps.check_cache.outputs.skip_dailymed_download != 'true'
        run: |
          # Extracts to production_data/dailymed_full_database.jsonl.gz
          # NOTE: Does NOT delete Zips yet, because Step 5 needs them!
          python3 production_data/extract_full_dailymed.py

      # --- STEP 4: Split & Commit Parts (Keep Full for Work) ---
      - name: 4. Split and Commit DailyMed Parts
        if: steps.check_cache.outputs.skip_dailymed_download != 'true'
        run: |
          echo "ðŸ”ª Splitting Database into 4 Parts..."
          PARTS_DIR="production_data/dailymed_full"
          # Ensure clean dir
          rm -rf "$PARTS_DIR"
          
          python3 scripts/manage_large_file.py split "production_data/dailymed_full_database.jsonl.gz" "$PARTS_DIR"
          
          git add -f "$PARTS_DIR/"
          git commit -m "data: dailymed full db split parts $(date +'%Y-%m-%d')" || echo "No changes"
          git pull --rebase -X theirs
          git push

      # --- STEP 5: Interactions (Matching) ---
      - name: 5. Extract Interactions (Matching)
        run: |
          echo "ðŸ§ª Running Interaction Extraction (Reads Cached Parts)..."
          # Can now run from Cache (Parts) OR Fresh Download
          python3 production_data/extract_interactions_production.py
          
      - name: 5.1 Commit Interactions
        run: |
          git add -f production_data/dailymed_interactions.json
          git commit -m "data: interactions extracted $(date +'%Y-%m-%d')" || echo "No changes"
          git pull --rebase -X theirs # Pull changes from Step 4
          git push

      # --- STEP 5.5: Cleanup Zips (Now safe) ---
      - name: 5.5 Cleanup Zips
        if: always()
        run: |
          echo "ðŸ§¹ Cleaning up Zips..."
          rm -rf External_source/dailymed/downloaded/*.zip

      # --- STEP 6: Dosages (Matching) ---
      - name: 6. Extract Dosages (Matching)
        # Runs whether cache hit or fresh download, because we have Reassembled DB in both cases
        run: |
          echo "ðŸ’Š Running Dosage Extraction (Reads Unified DB)..."
          # Reads production_data/dailymed_full_database.jsonl.gz
          python3 scripts/process_datalake.py

      - name: 6.1 Commit Dosages
        run: |
          git add -f production_data/production_dosages.jsonl
          git commit -m "data: dosages extracted $(date +'%Y-%m-%d')" || echo "No changes"
          git pull --rebase -X theirs
          git push

      # --- STEP 7: Finalize & Sync ---
      - name: 7. Merge Hybrid Data
        run: |
          python3 scripts/merge_hybrid_data.py

      - name: 7.1 Bootstrap Assets
        run: |
          python3 scripts/bootstrap_app_data.py
          
      - name: 7.2 Commit Final Assets
        run: |
          git add -f assets/data/interactions/
          
          git commit -m "db: full rebuild complete $(date +'%Y-%m-%d')" || echo "No changes"
          git pull --rebase -X theirs
          git push

      - name: 8. Sync to Cloudflare D1
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          D1_DATABASE_ID: ${{ secrets.D1_DATABASE_ID }}
        run: |
          echo "::group::Syncing Drugs Table"
          python3 scripts/export_to_d1.py assets/meds.csv d1_import.sql
          
          npm install -g wrangler
          cd cloudflare-worker
          
          # 1. Drugs
          npx wrangler d1 execute mediswitch-db --file=../d1_import.sql --remote
          
          # 2. Dosages (Generate SQL -> Execute)
          echo "::endgroup::"
          echo "::group::Syncing Dosages"
          python3 ../scripts/upload_dosage_d1.py ../assets/data/dosage_guidelines.json
          npx wrangler d1 execute mediswitch-db --file=d1_dosages.sql --remote
          echo "::endgroup::"
          
          # 3. Interactions (Generate SQL -> Execute)
          echo "::group::Syncing Interactions"
          python3 ../scripts/upload_interactions_d1.py ../assets/data/interactions/
          npx wrangler d1 execute mediswitch-db --file=d1_interactions.sql --remote
          echo "::endgroup::"

      - name: Final Notification
        uses: rtCamp/action-slack-notify@v2
        if: always()
        env:
            SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK_URL }}
            SLACK_MESSAGE: 'Database Rebuild Job Finished (Status: ${{ job.status }})'
