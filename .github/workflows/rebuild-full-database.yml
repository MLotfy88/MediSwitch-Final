name: Rebuild Full Database (Unified)
# The "Magic Button" to rebuild the entire database from scratch.
# Order:
# 1. Scrape DwaPrices (Details + Concentration) -> Update meds.csv
# 2. Download & Extract DailyMed (Full) -> Extract Interactions -> Cleanup Zips -> Split & Commit
# 3. Match & Produce Files (Drugs, Interactions, Dosages)
# 4. Update Local Assets (JSONs)
# 5. Sync D1 (Full Overwrite)

on:
  workflow_dispatch:
    inputs:
      force_scrape:
        description: 'Force Full Re-scrape (Delete local DB)?'
        required: false
        default: 'false'
        type: boolean
      test_mode:
        description: 'Test mode: Limit drugs for quick validation (default: 0 = all drugs)'
        required: false
        default: '0'
        type: string

jobs:
  full-rebuild:
    runs-on: ubuntu-latest
    timeout-minutes: 360 # 6 Hours Max
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          
      - name: Install Dependencies
        run: |
          pip install requests pandas beautifulsoup4 aiohttp aiofiles brotli

      # --- STEP 1: Scrape & Update CSV ---
      - name: 1. Scrape DwaPrices (Details + Concentration)
        continue-on-error: true
        run: |
          echo "::group::Scraping DwaPrices"
          # Test mode check
          LIMIT_FLAG=""
          if [ "${{ inputs.test_mode }}" != "0" ] && [ -n "${{ inputs.test_mode }}" ]; then
            LIMIT_FLAG="--limit ${{ inputs.test_mode }}"
            echo "ðŸ§ª TEST MODE: Limiting to ${{ inputs.test_mode }} drugs"
          fi
          
          # Requires meds.csv to exist in repo
          if [ "${{ inputs.force_scrape }}" == "true" ]; then
            echo "ðŸ”¥ FORCE SCRAPE ENABLED: Deleting old data..."
            python3 -u scripts/scrape_dwaprices_by_id.py --reset $LIMIT_FLAG
          else
            echo "â™»ï¸ SMART SCRAPE: Resuming/Updating only..."
            python3 -u scripts/scrape_dwaprices_by_id.py $LIMIT_FLAG
          fi
          echo "::endgroup::"
          
      - name: 1.5 Update meds.csv with Scraped Data
        run: |
          echo "::group::Updating CSV"
          python3 scripts/update_meds.py
          echo "::endgroup::"

      - name: 1.5 Commit Scraped Data
        if: always()
        run: |
          git config --global user.name 'DailyMed Bot'
          git config --global user.email 'bot@dailymed.local'
          
          git add -f assets/meds_scraped_new.jsonl
          git add -f assets/meds.csv
          
          git commit -m "data: scrape checkpoint $(date +'%Y-%m-%d')" || echo "No changes to commit"
          # FORCE PUSH to establish this as the new truth for this run
          git push --force-with-lease || git push

      # --- STEP 2: DailyMed (Download -> Extract -> Interactions -> Split -> Cleanup) ---
      - name: 2. Check DailyMed Cache (Chunks)
        id: check_dm_cache
        run: |
          # We check for the directory of parts
          CACHE_DIR="production_data/dailymed_cache"
          TARGET_PARTS="production_data/dailymed_full_database_parts"
          TARGET_FILE="production_data/dailymed_full_database.jsonl.gz"
          
          if [ -d "$CACHE_DIR/dailymed_full_database_parts" ]; then
             echo "âœ… Found valid cached DailyMed DB Parts"
             mkdir -p "$TARGET_PARTS"
             cp -r "$CACHE_DIR/dailymed_full_database_parts/." "$TARGET_PARTS/"
             
             # Reassemble immediately
             echo "ðŸ§© Reassembling Cached DB..."
             python3 scripts/manage_large_file.py join "$TARGET_PARTS" "$TARGET_FILE"
             
             echo "skip_dm=true" >> $GITHUB_OUTPUT
          else
             echo "âŒ No Valid DailyMed cache found."
             echo "skip_dm=false" >> $GITHUB_OUTPUT
          fi

      - name: 2.1 Download DailyMed
        if: steps.check_dm_cache.outputs.skip_dm != 'true'
        run: |
          echo "::group::Downloading DailyMed"
          python3 scripts/download_dailymed.py
          echo "::endgroup::"

      - name: 2.2 Extract DailyMed (Compressed)
        if: steps.check_dm_cache.outputs.skip_dm != 'true'
        run: |
          echo "::group::Extracting DailyMed XMLs"
          # Produces production_data/dailymed_full_database.jsonl.gz
          python3 production_data/extract_full_dailymed.py
          echo "::endgroup::"

      # REORDERED: Extract Interactions BEFORE cleaning Zips
      - name: 2.3 Extract DailyMed Interactions
        if: steps.check_dm_cache.outputs.skip_dm != 'true'
        run: |
          echo "::group::Extracting Interactions"
          # Reads Zips directly
          python3 production_data/extract_dailymed_interactions.py
          echo "::endgroup::"
          
          # Cache immediately
          CACHE_DIR="production_data/final_cache"
          mkdir -p "$CACHE_DIR"
          cp "production_data/dailymed_interactions.json" "$CACHE_DIR/"

      - name: 2.4 Cleanup Zips (Space Saver)
        if: always()
        run: |
          echo "ðŸ§¹ Cleaning up Downloaded Zips..."
          rm -rf External_source/dailymed/downloaded/*.zip

      - name: 2.5 Split, Cache & Commit DailyMed Database
        if: steps.check_dm_cache.outputs.skip_dm != 'true'
        run: |
          echo "ðŸ”ª Splitting Database into git-safe chunks..."
          python3 scripts/manage_large_file.py split "production_data/dailymed_full_database.jsonl.gz" "production_data/dailymed_full_database_parts"
          
          echo "ðŸ’¾ Caching Internal Parts..."
          CACHE_DIR="production_data/dailymed_cache"
          mkdir -p "$CACHE_DIR"
          cp -r "production_data/dailymed_full_database_parts" "$CACHE_DIR/"
          
          # Commit parts (NOT the large file)
          git add -f production_data/dailymed_full_database_parts/
          git commit -m "data: dailymed db chunks $(date +'%Y-%m-%d')" || echo "No changes"
          git pull --rebase -X theirs
          git push

      # --- STEP 3: Dosages (Extract -> Cache -> Clean) ---
      - name: 3.1 Check Dosages Cache
        id: check_dosages_cache
        run: |
          CACHE_DIR="production_data/final_cache"
          DOSAGES_CACHE="$CACHE_DIR/production_dosages.jsonl"
          TARGET_DOSAGES="production_data/production_dosages.jsonl"
          
          skip_dosages=false
          if [ -f "$DOSAGES_CACHE" ]; then
             echo "âœ… Found valid cached Dosages"
             cp "$DOSAGES_CACHE" "$TARGET_DOSAGES"
             skip_dosages=true
          fi
          echo "skip_dosages=$skip_dosages" >> $GITHUB_OUTPUT

      - name: 3.2 Extract Dosages
        if: steps.check_dosages_cache.outputs.skip_dosages != 'true'
        run: |
          # Reads production_data/dailymed_full_database.jsonl.gz (Guaranteed to exist from Step 2)
          python3 scripts/process_datalake.py
          
          # Save to Cache
          CACHE_DIR="production_data/final_cache"
          mkdir -p "$CACHE_DIR"
          cp "production_data/production_dosages.jsonl" "$CACHE_DIR/"
          echo "ðŸ’¾ Dosages cached"
          
      - name: 3.3 Commit Dosages & Interactions Checkpoint
        if: steps.check_dosages_cache.outputs.skip_dosages != 'true'
        run: |
          git add -f production_data/production_dosages.jsonl
          # Commit interactions here too if generated
          if [ -f "production_data/dailymed_interactions.json" ]; then
             git add -f production_data/dailymed_interactions.json
          fi
          git commit -m "data: dosages & interactions checkpoint $(date +'%Y-%m-%d')" || echo "No changes"
          git pull --rebase -X theirs
          git push

      # --- STEP 4: Merging & Finalizing ---
      - name: 3.8 Merge Hybrid Data
        run: |
          python3 scripts/merge_hybrid_data.py

      - name: 4. Bootstrap App Assets
        run: |
          python3 scripts/bootstrap_app_data.py
          
      - name: 4.5 Commit Final DB
        run: |
          git add -f assets/data/dosage_guidelines.json
          git add -f assets/data/drug_interactions.json
          git add -f assets/meds.csv
          
          git commit -m "db: full rebuild complete $(date +'%Y-%m-%d')" || echo "No changes"
          git pull --rebase -X theirs
          git push

      - name: 5. Sync Full Database to D1
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          D1_DATABASE_ID: ${{ secrets.D1_DATABASE_ID }}
        run: |
          echo "::group::Syncing Drugs Table"
          python3 scripts/export_to_d1.py assets/meds.csv d1_import.sql
          
          npm install -g wrangler
          cd cloudflare-worker
          npx wrangler d1 execute mediswitch-db --file=../d1_import.sql --remote
          echo "::endgroup::"
          
          echo "::group::Syncing Dosages (Full)"
          if [ -f "../scripts/upload_dosage_d1.py" ]; then
            python3 ../scripts/upload_dosage_d1.py \
              --json-file ../assets/data/dosage_guidelines.json \
              --account-id "$CLOUDFLARE_ACCOUNT_ID" \
              --database-id "$D1_DATABASE_ID" \
              --api-token "$CLOUDFLARE_API_TOKEN"
          fi
          echo "::endgroup::"
          
          echo "::group::Syncing Interactions (Full)"
          if [ -f "../scripts/upload_interactions_d1.py" ]; then
            python3 ../scripts/upload_interactions_d1.py \
              --json-file ../assets/data/drug_interactions.json \
              --account-id "$CLOUDFLARE_ACCOUNT_ID" \
              --database-id "$D1_DATABASE_ID" \
              --api-token "$CLOUDFLARE_API_TOKEN" \
              --clear-first
          fi
          echo "::endgroup::"

      - name: Final Notification
        uses: rtCamp/action-slack-notify@v2
        if: always()
        env:
            SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK_URL }}
            SLACK_MESSAGE: 'Database Rebuild Job Finished (Status: ${{ job.status }})'
