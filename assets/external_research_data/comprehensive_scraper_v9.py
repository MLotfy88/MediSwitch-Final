#!/usr/bin/env python3
"""
DDInter2 Comprehensive Scraper v9
==================================
Ø³ÙƒØ±Ø§Ø¨Ø± Ø´Ø§Ù…Ù„ Ù„Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆÙ‚Ø¹ DDInter2:
- Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø¯ÙˆÙŠØ© Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©
- ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-Ø¯ÙˆØ§Ø¡ (Drug-Drug)
- ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-Ù…Ø±Ø¶ (Drug-Disease)
- ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-ØºØ°Ø§Ø¡ (Drug-Food)
- Ø§Ù„Ù…Ø³ØªØ­Ø¶Ø±Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ¨Ø© (Compound Preparations)
"""

import requests
import sqlite3
import json
import re
import os
import time
import urllib3
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from bs4 import BeautifulSoup

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ============================================
# Configuration
# ============================================
DB_PATH = 'ddinter_complete.db'
SCHEMA_SQL = 'database_schema.sql'
DRUG_IDS_FILE = 'discovered_ids.json'
BASE_URL = 'https://ddinter2.scbdd.com'
MAX_WORKERS = 20
REQUEST_TIMEOUT = 15

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Referer': 'https://ddinter2.scbdd.com/',
    'Connection': 'keep-alive'
}

# ============================================
# Database Initialization
# ============================================
def init_database():
    """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ø¬Ø¯Ø§ÙˆÙ„"""
    print("ğŸ“¦ Initializing database...")
    
    if not os.path.exists(SCHEMA_SQL):
        print(f"âŒ Schema file not found: {SCHEMA_SQL}")
        return False
    
    conn = sqlite3.connect(DB_PATH)
    with open(SCHEMA_SQL, 'r', encoding='utf-8') as f:
        conn.executescript(f.read())
    conn.commit()
    conn.close()
    
    print(f"âœ… Database initialized: {DB_PATH}")
    return True

def load_drug_ids():
    """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ø£Ø¯ÙˆÙŠØ©"""
    if not os.path.exists(DRUG_IDS_FILE):
        print(f"âŒ Drug IDs file not found: {DRUG_IDS_FILE}")
        return []
    
    with open(DRUG_IDS_FILE, 'r') as f:
        data = json.load(f)
        drug_ids = data.get('unique_drugs', [])
        print(f"ğŸ“‹ Loaded {len(drug_ids)} drug IDs")
        return drug_ids

# ============================================
# HTML Parsing Utilities
# ============================================
def extract_table_value(soup, key_text):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ÙŠÙ…Ø© Ù…Ù† Ø¬Ø¯ÙˆÙ„ HTML Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØªØ§Ø­"""
    try:
        key_td = soup.find('td', class_='key', string=re.compile(key_text, re.I))
        if key_td:
            value_td = key_td.find_next_sibling('td', class_='value')
            if value_td:
                return value_td.get_text(strip=True)
    except:
        pass
    return None

def extract_atc_codes(soup):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ATC codes"""
    try:
        atc_row = soup.find('td', class_='key', string=re.compile('ATC Classification'))
        if atc_row:
            value_td = atc_row.find_next_sibling('td')
            badges = value_td.find_all('span', class_='badge')
            return [badge.get_text(strip=True) for badge in badges]
    except:
        pass
    return []

def extract_external_links(soup):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©"""
    try:
        links_row = soup.find('td', class_='key', string=re.compile('Useful Links'))
        if links_row:
            value_td = links_row.find_next_sibling('td')
            links = {}
            for a_tag in value_td.find_all('a'):
                name = a_tag.get_text(strip=True)
                url = a_tag.get('href', '')
                links[name] = url
            return links
    except:
        pass
    return {}

def parse_drug_drug_table(soup, drug_id):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø¯ÙˆÙ„ ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-Ø¯ÙˆØ§Ø¡"""
    interactions = []
    try:
        table = soup.find('table', id='interaction-table')
        if table:
            # Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ù„Ø¬Ø¯ÙˆÙ„ ÙŠØªÙ… Ù…Ù„Ø¤Ù‡ Ø¹Ø¨Ø± JavaScript/AJAX
            # Ù†Ø­ØªØ§Ø¬ Ù„Ø·Ù„Ø¨ API endpoint Ù…Ø¨Ø§Ø´Ø±Ø©
            # Ø³ÙŠØªÙ… Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù‡Ø°Ø§ ÙÙŠ scrape_drug_interactions
            pass
    except Exception as e:
        print(f"âš ï¸ Error parsing drug-drug table: {e}")
    return interactions

def parse_disease_table(soup):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø¯ÙˆÙ„ ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-Ù…Ø±Ø¶"""
    interactions = []
    try:
        table = soup.find('table', id='ddsi-table')
        if table and table.tbody:
            rows = table.tbody.find_all('tr')
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 4:
                    interactions.append({
                        'severity': cols[0].get_text(strip=True),
                        'disease_name': cols[1].get_text(strip=True),
                        'text': cols[2].get_text(strip=True),
                        'references': cols[3].get_text(strip=True)
                    })
    except Exception as e:
        print(f"âš ï¸ Error parsing disease table: {e}")
    return interactions

def parse_food_table(soup):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø¯ÙˆÙ„ ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-ØºØ°Ø§Ø¡"""
    interactions = []
    try:
        table = soup.find('table', id='dfi-table')
        if table and table.tbody:
            rows = table.tbody.find_all('tr')
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 6:
                    interactions.append({
                        'severity': cols[0].get_text(strip=True),
                        'food_name': cols[1].get_text(strip=True),
                        'description': cols[2].get_text(strip=True),
                        'management': cols[3].get_text(strip=True),
                        'mechanism': cols[4].get_text(strip=True),
                        'references': cols[5].get_text(strip=True)
                    })
    except Exception as e:
        print(f"âš ï¸ Error parsing food table: {e}")
    return interactions

# ============================================
# Phase 1: Scrape Drug Details
# ============================================
def scrape_drug_detail(drug_id):
    """Ø³Ø­Ø¨ ØªÙØ§ØµÙŠÙ„ Ø¯ÙˆØ§Ø¡ ÙˆØ§Ø­Ø¯"""
    url = f"{BASE_URL}/server/drug-detail/{drug_id}/"
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
        if response.status_code != 200:
            return None, f"HTTP {response.status_code}"
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        drug_data = {
            'ddinter_id': drug_id,
            'drug_name': soup.find('strong', string='Drugs Information:').next_sibling.strip() if soup.find('strong', string='Drugs Information:') else None,
            'drug_type': extract_table_value(soup, 'Drug Type'),
            'molecular_formula': extract_table_value(soup, 'Molecular Formula'),
            'molecular_weight': extract_table_value(soup, 'Molecular Weight'),
            'cas_number': extract_table_value(soup, 'CAS Number'),
            'description': extract_table_value(soup, 'Description'),
            'iupac_name': extract_table_value(soup, 'IUPAC Name'),
            'inchi': extract_table_value(soup, 'InChI'),
            'smiles': extract_table_value(soup, 'Canonical SMILES'),
            'atc_codes': json.dumps(extract_atc_codes(soup)),
            'external_links': json.dumps(extract_external_links(soup))
        }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª
        disease_interactions = parse_disease_table(soup)
        food_interactions = parse_food_table(soup)
        
        return {
            'drug': drug_data,
            'diseases': disease_interactions,
            'foods': food_interactions
        }, None
        
    except Exception as e:
        return None, str(e)

def save_drug_to_db(drug_data, disease_interactions, food_interactions):
    """Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ø¡ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    
    try:
        # Ø­ÙØ¸ Ø§Ù„Ø¯ÙˆØ§Ø¡
        c.execute('''
            INSERT OR REPLACE INTO drugs 
            (ddinter_id, drug_name, drug_type, molecular_formula, molecular_weight, 
             cas_number, description, iupac_name, inchi, smiles, atc_codes, external_links)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            drug_data['ddinter_id'], drug_data['drug_name'], drug_data['drug_type'],
            drug_data['molecular_formula'], drug_data['molecular_weight'],
            drug_data['cas_number'], drug_data['description'], drug_data['iupac_name'],
            drug_data['inchi'], drug_data['smiles'], drug_data['atc_codes'],
            drug_data['external_links']
        ))
        
        # Ø­ÙØ¸ ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ù…Ø±Ø¶
        for interaction in disease_interactions:
            c.execute('''
                INSERT INTO drug_disease_interactions 
                (drug_id, disease_name, severity, interaction_text, reference_text)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                drug_data['ddinter_id'], interaction['disease_name'], 
                interaction['severity'], interaction['text'], interaction['references']
            ))
        
        # Ø­ÙØ¸ ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„ØºØ°Ø§Ø¡
        for interaction in food_interactions:
            c.execute('''
                INSERT INTO drug_food_interactions 
                (drug_id, food_name, severity, description, management, mechanism_flags, reference_text)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                drug_data['ddinter_id'], interaction['food_name'], 
                interaction['severity'], interaction['description'],
                interaction['management'], interaction['mechanism'], interaction['references']
            ))
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙ‚Ø¯Ù…
        c.execute('''
            INSERT OR REPLACE INTO scraping_progress (entity_type, entity_id, status)
            VALUES ('drug', ?, 'completed')
        ''', (drug_data['ddinter_id'],))
        
        conn.commit()
        return True
        
    except Exception as e:
        conn.rollback()
        print(f"âŒ Error saving {drug_data['ddinter_id']}: {e}")
        return False
    finally:
        conn.close()

# ============================================
# Phase 2: Scrape Drug-Drug Interactions
# ============================================
def scrape_drug_interactions(start_id=1, end_id=60000):
    """Ø³Ø­Ø¨ ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-Ø¯ÙˆØ§Ø¡ (Ù…Ù† Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± v8 Ø§Ù„Ù†Ø§Ø¬Ø­)"""
    print(f"\nğŸ”„ Phase 2: Scraping drug-drug interactions ({start_id}-{end_id})...")
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙØ³ Ù…Ù†Ø·Ù‚ bulk_scraper_v8_html.py Ø§Ù„Ù†Ø§Ø¬Ø­
    # TODO: Ø¯Ù…Ø¬ Ù…Ù†Ø·Ù‚ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± v8 Ù‡Ù†Ø§
    pass

# ============================================
# Main Execution
# ============================================
def main():
    print("="*60)
    print("ğŸš€ DDInter2 Comprehensive Scraper v9")
    print("="*60)
    
    # 1. Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    if not init_database():
        return
    
    # 2. ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø¯ÙˆÙŠØ©
    drug_ids = load_drug_ids()
    if not drug_ids:
        print("âŒ No drug IDs to process")
        return
    
    # 3. Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø³Ø­Ø¨ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø£Ø¯ÙˆÙŠØ©
    print(f"\nğŸ”„ Phase 1: Scraping {len(drug_ids)} drugs...")
    
    success_count = 0
    error_count = 0
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(scrape_drug_detail, drug_id): drug_id for drug_id in drug_ids[:100]}  # Test with 100 first
        
        for future in as_completed(futures):
            drug_id = futures[future]
            try:
                result, error = future.result()
                if result:
                    if save_drug_to_db(result['drug'], result['diseases'], result['foods']):
                        success_count += 1
                        if success_count % 10 == 0:
                            print(f"âœ… Progress: {success_count} drugs processed")
                else:
                    error_count += 1
                    print(f"âš ï¸ Failed {drug_id}: {error}")
            except Exception as e:
                error_count += 1
                print(f"âŒ Error processing {drug_id}: {e}")
    
    print(f"\nâœ… Phase 1 Complete: {success_count} success, {error_count} errors")
    
    # 4. Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø³Ø­Ø¨ ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-Ø¯ÙˆØ§Ø¡
    # scrape_drug_interactions()
    
    print("\n" + "="*60)
    print("ğŸ‰ Scraping Complete!")
    print("="*60)

if __name__ == "__main__":
    main()
