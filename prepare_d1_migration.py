import sqlite3
import os
import json

DB_PATH = "mediswitch.db"
OUTPUT_DIR = "d1_migration_sql"

def export_for_d1():
    """ุชุตุฏูุฑ ุงูุจูุงูุงุช ุจุตูุบุฉ SQL ููุงุณุจุฉ ูู Cloudflare D1"""
    
    if not os.path.exists(DB_PATH):
        print(f"โ Database not found: {DB_PATH}")
        return
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    print("๐ ุชุญุถูุฑ ุงูุจูุงูุงุช ูููุฒุงููุฉ ูุน Cloudflare D1")
    print("="*80)
    
    # 1. Schema ููุท (ุงูุฌุฏุงูู ูุงูู Indexes)
    print("\n๐ ุฎุทูุฉ 1: ุชุตุฏูุฑ Schema...")
    cursor.execute("SELECT sql FROM sqlite_master WHERE type IN ('table', 'index') AND sql IS NOT NULL")
    schema_sql = []
    for row in cursor.fetchall():
        schema_sql.append(row[0] + ";")
    
    with open(f"{OUTPUT_DIR}/01_schema.sql", "w", encoding="utf-8") as f:
        f.write("-- Cloudflare D1 Schema\n")
        f.write("-- Generated for MediSwitch Database\n\n")
        f.write("\n\n".join(schema_sql))
    print(f"   โ ุชู ุญูุธ Schema ูู 01_schema.sql")
    
    # 2. ูุนูููุงุช ุงูุญุฌู
    tables_info = []
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'")
    for (table_name,) in cursor.fetchall():
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count = cursor.fetchone()[0]
        tables_info.append({
            "table": table_name,
            "rows": count,
            "size_mb": 0  # ุณูุชู ุญุณุงุจู ูุงุญูุงู
        })
    
    # 3. ุฎุทุฉ ุงูุชูุณูู
    print("\n๐ ุฎุทูุฉ 2: ุชุญููู ุญุฌู ุงูุจูุงูุงุช...")
    plan = {
        "total_tables": len(tables_info),
        "tables": tables_info,
        "strategy": "incremental",
        "notes": [
            "โ๏ธ ุญุฌู ูุงุนุฏุฉ ุงูุจูุงูุงุช ูุจูุฑ ุฌุฏุงู (6.2 GB)",
            "๐ก ุงูุญู: ุงุณุชุฎุฏุงู wrangler d1 execute ูุน batch inserts",
            "๐ฆ ุชูุณูู ูู ุฌุฏูู ูู chunks ุตุบูุฑุฉ (1000 row/chunk)",
            "โก ุฑูุน ูู chunk ุนูู ุญุฏุฉ"
        ]
    }
    
    with open(f"{OUTPUT_DIR}/migration_plan.json", "w", encoding="utf-8") as f:
        json.dump(plan, f, indent=2, ensure_ascii=False)
    
    # 4. ุนุฑุถ ุงูููุฎุต
    print("\n" + "="*80)
    print("๐ ููุฎุต ุงูุจูุงูุงุช:")
    print("-"*80)
    for info in sorted(tables_info, key=lambda x: x['rows'], reverse=True):
        print(f"   {info['table']:30} : {info['rows']:>12,} rows")
    
    print("\n" + "="*80)
    print("โ๏ธ  ุชุญุฐูุฑ ููู:")
    print("-"*80)
    print("   ูุงุนุฏุฉ ุงูุจูุงูุงุช ูุจูุฑุฉ ุฌุฏุงู (6.2 GB)")
    print("   Cloudflare D1 ุนูุฏู ุญุฏูุฏ:")
    print("   - ุญุฏ ุฃูุตู 10 GB ููู database")
    print("   - ุญุฏ ุฃูุตู 100,000 ุตู ููู batch insert")
    print("   - ุญุฏ ุฃูุตู 1 MB ููู SQL statement")
    
    print("\n๐ก ุงูุฎูุงุฑุงุช ุงููุชุงุญุฉ:")
    print("-"*80)
    print("   1๏ธโฃ  ุฑูุน ุจูุงูุงุช ุงูุฃุฏููุฉ + ุชูุงุนูุงุช ุงูุฃุฏููุฉ ููุท (ุงูุฃุณุงุณูุงุช)")
    print("   2๏ธโฃ  ุชูููู ุชูุงุนูุงุช ุงูุฃูุฑุงุถ (ุญุงููุงู 7.7 ููููู โ ูุฎูููุง ุญุณุจ ุงูุฏูุงุก ุงูุฃุณุงุณู ููุท)")
    print("   3๏ธโฃ  ุงุณุชุฎุฏุงู External Storage ูุชูุงุนูุงุช ุงูุฃูุฑุงุถ ูุงูุทุนุงู")
    
    conn.close()
    
    print("\nโ ุงูุชูู ุงูุชุญุถูุฑ. ุงููููุงุช ูู: " + OUTPUT_DIR)
    return tables_info

if __name__ == "__main__":
    export_for_d1()
