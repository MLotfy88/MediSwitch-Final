name: Resume Rebuild (Skip Scraping)
# Uses existing meds.csv / scraped data in the repo and just rebuilds the robust data.

on:
  workflow_dispatch:

jobs:
  resume-build:
    runs-on: ubuntu-latest
    timeout-minutes: 360 # 6 Hours Max
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          
      - name: Install Dependencies
        run: |
          pip install requests pandas beautifulsoup4 aiohttp aiofiles brotli

      # --- STEP 1.0: Check for Existing Scraped Data ---
      - name: 1.0 Check for Existing Drug Data
        id: check_scrape
        run: |
          csv_file="assets/meds.csv"
          jsonl_file="assets/meds_scraped_new.jsonl"
          
          # Check CSV lines
          csv_lines=0
          if [ -f "$csv_file" ]; then
             csv_lines=$(wc -l < "$csv_file")
          fi
          
          # Check JSONL size
          jsonl_size=0
          if [ -f "$jsonl_file" ]; then
             jsonl_size=$(stat -c%s "$jsonl_file")
          fi
          
          echo "üìä Current Data Status:"
          echo "   - meds.csv lines: $csv_lines"
          echo "   - scraped jsonl size: $jsonl_size bytes"
          
          # Logic: If we have a healthy CSV (>100 lines) OR a healthy Scrape file (>10KB), we assume resume is possible.
          # If BOTH are weak, we MUST scrape.
          
          if [ "$csv_lines" -gt 100 ] || [ "$jsonl_size" -gt 10000 ]; then
             echo "‚úÖ Found valid existing drug data."
             echo "üöÄ SKIPPING Scraper!"
             echo "skip_scrape=true" >> $GITHUB_OUTPUT
          else
             echo "‚ö†Ô∏è No valid drug data found. Scraper execution REQUIRED."
             echo "skip_scrape=false" >> $GITHUB_OUTPUT
          fi

      # --- STEP 1.1: Scrape DwaPrices (Conditional) ---
      - name: 1.1 Scrape DwaPrices (If Data Missing)
        if: steps.check_scrape.outputs.skip_scrape != 'true'
        continue-on-error: true
        run: |
          echo "::group::Scraping DwaPrices (Cleanup Required)"
          # Since file is missing/bad, allow scraper to run in update mode (resumes or starts fresh if empty)
          python3 -u scripts/scrape_dwaprices_by_id.py
          echo "::endgroup::"

      # --- STEP 1.5: Ensure CSV is Up-to-Date ---
      - name: 1.5 Update meds.csv with Scraped Data (If exists)
        run: |
          echo "::group::Updating CSV from Repo Data"
          ls -l scripts/
          # If user pushed meds_scraped_new.jsonl OR scraper just created it
          if [ -f "assets/meds_scraped_new.jsonl" ]; then
             echo "‚úÖ Found scraped data file. Updating meds.csv..."
             python3 scripts/update_meds.py
          else
             echo "‚ö†Ô∏è No scraped data file found. Using existing meds.csv as is."
             python3 scripts/validate_linked_db.py || echo "Warning: Validation failed"
          fi
          echo "::endgroup::"

      - name: 1.6 Commit Updated CSV (If changed)
        run: |
          if [ -f "assets/meds_scraped_new.jsonl" ]; then
             echo "üíæ Scraped data source found. Committing updated CSV..."
             git config --global user.name 'DailyMed Bot'
             git config --global user.email 'bot@dailymed.local'
             
             git add -f assets/meds.csv
             git add -f assets/meds_backup.csv
             
             git commit -m "data: update meds.csv from uploaded jsonl $(date +'%Y-%m-%d')" || echo "No changes"
             git pull --rebase -X theirs
             git push
          fi

      # --- STEP 1.7: Check for Existing Processed Data (Smart Resume) ---
      - name: 1.7 Check for Existing Intermediate Data
        id: check_files
        run: |
          dosages="production_data/production_dosages.jsonl"
          interactions="production_data/dailymed_interactions.json"
          
          if [ -f "$dosages" ] && [ -f "$interactions" ]; then
             lines=$(wc -l < "$dosages")
             size=$(stat -c%s "$interactions")
             
             if [ "$lines" -gt 100 ] && [ "$size" -gt 1000 ]; then
                echo "‚úÖ Found valid existing data ($lines lines, $size bytes)."
                echo "üöÄ SKIPPING DailyMed Download & Extraction!"
                echo "skip_dailymed=true" >> $GITHUB_OUTPUT
             else
                echo "‚ö†Ô∏è Data files exist but seem small/empty. Re-running processing."
                echo "skip_dailymed=false" >> $GITHUB_OUTPUT
             fi
          else
             echo "‚ö†Ô∏è Intermediate files validation failed. Re-running processing."
             echo "skip_dailymed=false" >> $GITHUB_OUTPUT
          fi

      # --- STEP 2: DailyMed Download ---
      - name: 2. Download DailyMed Full Release
        if: steps.check_files.outputs.skip_dailymed != 'true'
        run: |
          echo "::group::Downloading DailyMed"
          python3 scripts/download_dailymed.py
          echo "::endgroup::"

      - name: 2.5 Extract DailyMed Data Lake
        if: steps.check_files.outputs.skip_dailymed != 'true'
        run: |
          echo "::group::Extracting DailyMed XMLs"
          python3 production_data/extract_full_dailymed.py
          echo "::endgroup::"

      # --- STEP 3: Extraction & Matching ---
      - name: 3.1 Extract Dosages (Process Data Lake)
        if: steps.check_files.outputs.skip_dailymed != 'true'
        run: |
          python3 scripts/process_datalake.py
          
      - name: 3.2 Extract Interactions
        if: steps.check_files.outputs.skip_dailymed != 'true'
        run: |
          python3 production_data/extract_dailymed_interactions.py

      - name: 3.2.5 Commit Intermediate Data (Dosages & Interactions)
        run: |
          dosages="production_data/production_dosages.jsonl"
          interactions="production_data/dailymed_interactions.json"
          
          # Check files exist
          if [ ! -f "$dosages" ] || [ ! -f "$interactions" ]; then
             echo "‚ö†Ô∏è Intermediate files missing. Skipping commit."
             exit 0
          fi
          
          # Check size/content
          lines=$(wc -l < "$dosages")
          size=$(stat -c%s "$interactions")
          
          echo "üìä Dosages Lines: $lines"
          echo "üìä Interactions Size: $size bytes"
          
          if [ "$lines" -gt 0 ] && [ "$size" -gt 100 ]; then
             echo "‚úÖ Valid data found. Committing..."
             git config --global user.name 'DailyMed Bot'
             git config --global user.email 'bot@dailymed.local'
             
             git add -f "$dosages"
             git add -f "$interactions"
             
             git commit -m "data: save intermediate dosages/interactions $(date +'%Y-%m-%d')" || echo "No changes"
             git pull --rebase -X theirs
             git push
          else
             echo "‚ö†Ô∏è Data appears empty (Lines=0 or Size<100). Skipping commit."
          fi

      - name: 3.3 Merge Hybrid Data
        run: |
          python3 scripts/merge_hybrid_data.py

      # --- STEP 4: Asset Generation ---
      - name: 4. Bootstrap App Assets (Generate JSONs)
        run: |
          python3 scripts/bootstrap_app_data.py

      # --- COMMIT POINT ---
      - name: Commit Updated Database
        run: |
          git config --global user.name 'DailyMed Bot'
          git config --global user.email 'bot@dailymed.local'
          
          # 1. Add everything (respecting .gitignore which now excludes large files)
          git add .
          
          # 2. FORCE ADD the final assets we actually want (even if ignored)
          git add -f assets/data/dosage_guidelines.json
          git add -f assets/data/drug_interactions.json
          git add -f assets/meds.csv
          
          # Only commit if changes exist
          git commit -m "db: resume rebuild $(date +'%Y-%m-%d')" || echo "No changes to commit"
          
          # Rebase favoring OUR changes (the newly generated data) over upstream
          git pull --rebase -X theirs
          git push

      # --- STEP 5: D1 Sync (Full) ---
      - name: 5. Sync Full Database to D1
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID || '9f7fd7dfef294f26d47d62df34726367' }}
          D1_DATABASE_ID: ${{ secrets.D1_DATABASE_ID || '77da23cd-a8cc-40bf-9c0f-f0effe7eeaa0' }}
        run: |
          echo "::group::Syncing Drugs Table"
          python3 scripts/export_to_d1.py assets/meds.csv d1_import.sql
          
          npm install -g wrangler
          cd cloudflare-worker
          npx wrangler d1 execute mediswitch-db --file=../d1_import.sql --remote
          echo "::endgroup::"
          
          echo "::group::Syncing Dosages (Full)"
          # (Assuming upload_dosage_d1.py handles the update)
          python3 ../scripts/upload_dosage_d1.py \
            --json-file ../assets/data/dosage_guidelines.json \
            --account-id "$CLOUDFLARE_ACCOUNT_ID" \
            --database-id "$D1_DATABASE_ID" \
            --api-token "$CLOUDFLARE_API_TOKEN"
          echo "::endgroup::"
          
          echo "::group::Syncing Interactions (Full)"
          python3 ../scripts/upload_interactions_d1.py \
            --json-file ../assets/data/drug_interactions.json \
            --account-id "$CLOUDFLARE_ACCOUNT_ID" \
            --database-id "$D1_DATABASE_ID" \
            --api-token "$CLOUDFLARE_API_TOKEN" \
            --clear-first
          echo "::endgroup::"
