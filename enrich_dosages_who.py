import json
import sqlite3
import pandas as pd
import os
import time
import gzip
import re

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
WHO_CSV = "assets/external_research_data/WHO_ATC_DDD_2024.csv"
DOSAGE_JSON = os.path.join(BASE_DIR, 'assets', 'data', 'dosage_guidelines.json.gz')
DB_PATH = "mediswitch.db"

def clean_name(name):
    """ØªÙ†Ø¸ÙŠÙ Ø§Ø³Ù… Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„ÙØ¹Ø§Ù„Ø© (Ù†ÙØ³ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙÙŠ populate_mediswitch_final.py)"""
    if not name: return ""
    name = re.sub(r'\(.*?\)', '', name)
    salts = ['tromethamine', 'sodium', 'potassium', 'hcl', 'hydrochloride', 'maleate', 'sulfate', 'phosphate', 'fumarate', 'citrate', 'calcium', 'magnesium', 'acetate', 'topical', 'systemic']
    name = name.lower().strip()
    for salt in salts:
        name = name.replace(f" {salt}", "").replace(f"{salt} ", "").strip()
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„Ø±Ù…ÙˆØ²
    name = re.sub(r'[,;.\-\s]+', ' ', name).strip()
    return name

def enrich_data_high_fidelity():
    if not os.path.exists(WHO_CSV):
        print(f"âŒ Ù…Ù„Ù WHO ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {WHO_CSV}")
        return

    # Create empty list if dosage file doesn't exist
    if not os.path.exists(DOSAGE_JSON):
        print("âš ï¸ Ù…Ù„Ù Ø§Ù„Ø¬Ø±Ø¹Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡.")
        dosage_data = [] # Will be populated later or used as base
    else:
        # Check happens later in loading block
        pass

    # --- 0. ØªØ¬Ù…ÙŠØ¹ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù„Ø¶Ù…Ø§Ù† ÙˆØ¬ÙˆØ¯Ù‡Ø§) ---
    print("ğŸ§© ØªØ¬Ù…ÙŠØ¹ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡...")
    base_dir = os.path.dirname(os.path.abspath(__file__))
    parts_dir = os.path.join(base_dir, 'assets', 'database', 'parts')
    temp_db_path = os.path.join(base_dir, 'mediswitch.db')
    
    # Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù†Ø¬Ù…Ø¹ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø·Ø§Ø²Ø¬Ø©
    if os.path.exists(temp_db_path):
        os.remove(temp_db_path)
    
    parts = sorted([f for f in os.listdir(parts_dir) if f.startswith('mediswitch.db.part-')])
    if not parts:
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!")
        return
        
    with open(temp_db_path, 'wb') as outfile:
        for part in parts:
            part_path = os.path.join(parts_dir, part)
            with open(part_path, 'rb') as infile:
                outfile.write(infile.read())
    print(f"âœ… ØªÙ… ØªØ¬Ù…ÙŠØ¹ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {temp_db_path}")

    print("ğŸ”— Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©...")
    conn = sqlite3.connect(temp_db_path)
    conn.execute("PRAGMA busy_timeout = 10000")
    c = conn.cursor()

    # --- 1. Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ø¯ÙˆÙŠØ© Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ù…Ù† Ø¬Ø¯ÙˆÙ„ med_ingredients ---
    # Ù†Ø³ØªØ®Ø¯Ù… med_ingredients Ù„Ø£Ù†Ù‡ Ø£Ø¯Ù‚ ÙˆÙŠØ±Ø¨Ø· ÙƒÙ„ Ù…Ø§Ø¯Ø© ÙØ¹Ø§Ù„Ø© Ø¨Ø§Ù„Ù€ ID
    c.execute("SELECT med_id, ingredient FROM med_ingredients")
    local_drug_map = {}
    
    for med_id, ingredient in c.fetchall():
        if not ingredient: continue
        cleaned = clean_name(ingredient)
        if not cleaned: continue
        
        if cleaned not in local_drug_map:
            local_drug_map[cleaned] = []
        # ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± Ù„Ù†ÙØ³ Ø§Ù„Ø¯ÙˆØ§Ø¡
        if med_id not in local_drug_map[cleaned]:
            local_drug_map[cleaned].append(med_id)
            
    print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(local_drug_map):,} Ù…Ø§Ø¯Ø© ÙØ¹Ø§Ù„Ø© Ù…Ø­Ù„ÙŠØ© (Ù…Ù† med_ingredients).")

    # --- 2. ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¬Ø±Ø¹Ø§Øª JSON Ù„Ù„ØªØ­Ø¯ÙŠØ« ---
    with open(DOSAGE_JSON, 'r', encoding='utf-8') as f:
        dosage_data = json.load(f)
    
    # Ø®Ø±ÙŠØ·Ø© Ù„Ù…Ø¹Ø±ÙØ© Ù‡Ù„ ÙŠÙˆØ¬Ø¯ Ø³Ø¬Ù„ WHO Ù…Ø³Ø¨Ù‚Ø§Ù‹ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
    # (med_id, atc_code, ddd, route) -> record
    who_existing_map = {}
    for g in dosage_data:
        if g.get('source') == 'WHO ATC/DDD 2024':
            who_existing_map[(g['med_id'], g.get('atc_code'), g.get('min_dose'), g.get('route_code'))] = g

    # Ø£Ø¹Ù„Ù‰ ID Ù…Ø³ØªØ®Ø¯Ù…
    max_id = max([g.get('id', 0) for g in dosage_data]) if dosage_data else 0

    # --- 3. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª WHO ÙˆÙ…Ø·Ø§Ø¨Ù‚ØªÙ‡Ø§ ---
    print("\nğŸ§ª Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨ÙŠØ§Ù†Ø§Øª WHO ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„...")
    atc_update_count = 0
    added_count = 0
    
    with open(WHO_CSV, mode='r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            if row['ddd'] == 'NA' and row['atc_name'] == 'NA': continue
            
            who_drug_name = row['atc_name']
            who_atc = row['atc_code']
            ddd_val = row['ddd']
            adm_r = row['adm_r']
            
            cleaned_who = clean_name(who_drug_name)
            
            # Ù…Ù†Ø·Ù‚ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
            matched_ids = local_drug_map.get(cleaned_who, [])
            
            if not matched_ids and len(cleaned_who) >= 4:
                for local_clean, ids in local_drug_map.items():
                    if len(local_clean) >= 4:
                        if cleaned_who in local_clean or local_clean in cleaned_who:
                            matched_ids = ids
                            break
            
            if matched_ids:
                for local_id in matched_ids:
                    # Ø£. ØªØ­Ø¯ÙŠØ« ÙƒÙˆØ¯ ATC
                    c.execute("UPDATE drugs SET atc_codes = ? WHERE id = ? AND (atc_codes IS NULL OR atc_codes = '')", (who_atc, local_id))
                    if c.rowcount > 0: atc_update_count += 1
                    
                    # Ø¨. Ø¥Ø¶Ø§ÙØ© Ø³Ø¬Ù„ WHO Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹ Ù„Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙˆØ§Ø¡ (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ med_id + code + ddd + route)
                    try:
                        numeric_ddd = float(ddd_val) if ddd_val != 'NA' else None
                    except: numeric_ddd = None

                    if ddd_val != 'NA' and (local_id, who_atc, numeric_ddd, adm_r) not in who_existing_map:
                        uom = row['uom']
                        note = row['note']
                        
                        route_map = {'O': 'Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„ÙÙ…', 'P': 'Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø­Ù‚Ù†', 'R': 'Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø´Ø±Ø¬', 'V': 'Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ù…Ù‡Ø¨Ù„', 'Inhal': 'Ø§Ø³ØªÙ†Ø´Ø§Ù‚', 'N': 'Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø£Ù†Ù', 'TD': 'Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø¬Ù„Ø¯'}
                        route_ar = route_map.get(adm_r, adm_r)
                        
                        max_id += 1
                        new_g = {
                            "id": max_id,
                            "med_id": local_id,
                            "dailymed_setid": "N/A",
                            "min_dose": numeric_ddd,
                            "max_dose": None,
                            "frequency": 24,
                            "duration": 7,
                            "instructions": f"Ø§Ù„Ø¬Ø±Ø¹Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© (WHO DDD): {ddd_val} {uom} ({route_ar}). {note if note != 'NA' else ''}".strip(),
                            "condition": "General",
                            "source": "WHO ATC/DDD 2024",
                            "is_pediatric": 0,
                            "atc_code": who_atc,
                            "route_code": adm_r
                        }
                        dosage_data.append(new_g)
                        who_existing_map[(local_id, who_atc, numeric_ddd, adm_r)] = new_g
                        added_count += 1
    
    # Ø­ÙØ¸ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª
    conn.commit()
    conn.close()
    
    with gzip.open(DOSAGE_JSON, 'wt', encoding='utf-8') as f:
        json.dump(dosage_data, f, ensure_ascii=False, separators=(',', ':'))

    print(f"\nâœ¨ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:")
    print(f"ğŸ”¹ ØªÙ… ØªØ­Ø¯ÙŠØ« Ø£ÙƒÙˆØ§Ø¯ ATC Ù„Ù€ {atc_update_count:,} Ø¯ÙˆØ§Ø¡.")
    print(f"ğŸ”¹ ØªÙ… Ø¥Ø¶Ø§ÙØ© {added_count:,} Ø³Ø¬Ù„ Ø¬Ø±Ø¹Ø§Øª Ø¬Ø¯ÙŠØ¯ Ù…Ù† WHO.")
    print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª ÙÙŠ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ùˆ {DOSAGE_JSON}")

    # --- 4. Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‚Ø³ÙŠÙ… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø­ÙØ¸ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª ---
    print("ğŸ§© Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‚Ø³ÙŠÙ… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù„Ø­ÙØ¸Ù‡Ø§ ÙÙŠ Git...")
    split_database(temp_db_path, parts_dir)

def split_database(input_file, output_dir, chunk_size=50*1024*1024): # 50MB matches existing parts roughly
    """Split the DB back into parts matching 'split' command naming (aa, ab, ...)"""
    if not os.path.exists(input_file): return
    
    # Clean old parts
    for f in os.listdir(output_dir):
        if f.startswith('mediswitch.db.part-'):
            os.remove(os.path.join(output_dir, f))
            
    # Generate suffixes: aa, ab, ac...
    import string
    chars = string.ascii_lowercase
    suffixes = []
    for c1 in chars:
        for c2 in chars:
            suffixes.append(c1 + c2)
            
    part_num = 0
    with open(input_file, 'rb') as infile:
        while True:
            chunk = infile.read(chunk_size)
            if not chunk: break
            
            if part_num >= len(suffixes):
                print("âŒ Too many parts!")
                break
                
            suffix = suffixes[part_num]
            filename = f"mediswitch.db.part-{suffix}"
            output_path = os.path.join(output_dir, filename)
            
            with open(output_path, 'wb') as outfile:
                outfile.write(chunk)
            
            print(f"  âœ… Created {filename}")
            part_num += 1
            
    print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {part_num} Ø¬Ø²Ø¡ Ø¨Ù†Ø¬Ø§Ø­.")

if __name__ == "__main__":
    enrich_data_high_fidelity()
