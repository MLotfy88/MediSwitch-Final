name: Rebuild Full Database (Unified)
# The "Magic Button" to rebuild the entire database from scratch.
# Order:
# 1. Scrape DwaPrices (Details + Concentration) -> Update meds.csv
# 2. Download & Extract DailyMed (Full) -> GZIP -> Cleanup Zips
# 3. Match & Produce Files (Drugs, Interactions, Dosages)
# 4. Update Local Assets (JSONs)
# 5. Sync D1 (Full Overwrite)

on:
  workflow_dispatch:
    inputs:
      force_scrape:
        description: 'Force Full Re-scrape (Delete local DB)?'
        required: false
        default: 'false'
        type: boolean
      test_mode:
        description: 'Test mode: Limit drugs for quick validation (default: 0 = all drugs)'
        required: false
        default: '0'
        type: string

jobs:
  full-rebuild:
    runs-on: ubuntu-latest
    timeout-minutes: 360 # 6 Hours Max
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          
      - name: Install Dependencies
        run: |
          pip install requests pandas beautifulsoup4 aiohttp aiofiles brotli

      # --- STEP 1: Scrape & Update CSV ---
      - name: 1. Scrape DwaPrices (Details + Concentration)
        continue-on-error: true
        run: |
          echo "::group::Scraping DwaPrices"
          # Test mode check
          LIMIT_FLAG=""
          if [ "${{ inputs.test_mode }}" != "0" ] && [ -n "${{ inputs.test_mode }}" ]; then
            LIMIT_FLAG="--limit ${{ inputs.test_mode }}"
            echo "ðŸ§ª TEST MODE: Limiting to ${{ inputs.test_mode }} drugs"
          fi
          
          # Requires meds.csv to exist in repo
          if [ "${{ inputs.force_scrape }}" == "true" ]; then
            echo "ðŸ”¥ FORCE SCRAPE ENABLED: Deleting old data..."
            python3 -u scripts/scrape_dwaprices_by_id.py --reset $LIMIT_FLAG
          else
            echo "â™»ï¸ SMART SCRAPE: Resuming/Updating only..."
            python3 -u scripts/scrape_dwaprices_by_id.py $LIMIT_FLAG
          fi
          echo "::endgroup::"
          
      - name: 1.5 Update meds.csv with Scraped Data
        run: |
          echo "::group::Updating CSV"
          python3 scripts/update_meds.py
          echo "::endgroup::"

      - name: 1.5 Commit Scraped Data
        if: always()
        run: |
          git config --global user.name 'DailyMed Bot'
          git config --global user.email 'bot@dailymed.local'
          
          git add -f assets/meds_scraped_new.jsonl
          git add -f assets/meds.csv
          
          git commit -m "data: scrape checkpoint $(date +'%Y-%m-%d')" || echo "No changes to commit"
          # FORCE PUSH to establish this as the new truth for this run
          # This prevents old remote schema from merging back in
          git push --force-with-lease || git push

      # --- STEP 2: DailyMed (Download -> Extract -> Cache -> Clean) ---
      - name: 2. Check DailyMed Cache
        id: check_dm_cache
        run: |
          # We now cache the GZIP file
          CACHE_FILE="production_data/dailymed_cache/dailymed_full_database.jsonl.gz"
          TARGET_FILE="production_data/dailymed_full_database.jsonl.gz"
          
          if [ -f "$CACHE_FILE" ]; then
             size=$(stat -c%s "$CACHE_FILE")
             if [ "$size" -gt 10000000 ]; then # >10MB check (it should be bigger)
                echo "âœ… Found valid cached DailyMed DB ($size bytes)"
                cp "$CACHE_FILE" "$TARGET_FILE"
                echo "skip_dm=true" >> $GITHUB_OUTPUT
             else
                echo "âš ï¸ Cache file too small ($size bytes), re-downloading..."
                echo "skip_dm=false" >> $GITHUB_OUTPUT
             fi
          else
             echo "âŒ No DailyMed cache found."
             echo "skip_dm=false" >> $GITHUB_OUTPUT
          fi

      - name: 2.1 Download DailyMed
        if: steps.check_dm_cache.outputs.skip_dm != 'true'
        run: |
          echo "::group::Downloading DailyMed"
          python3 scripts/download_dailymed.py
          echo "::endgroup::"

      - name: 2.2 Extract DailyMed (Compressed)
        if: steps.check_dm_cache.outputs.skip_dm != 'true'
        run: |
          echo "::group::Extracting DailyMed XMLs"
          # Produces production_data/dailymed_full_database.jsonl.gz DIRECTLY
          python3 production_data/extract_full_dailymed.py
          echo "::endgroup::"

      - name: 2.3 Cleanup Zips & Cache Database
        if: steps.check_dm_cache.outputs.skip_dm != 'true'
        run: |
          echo "ðŸ§¹ Cleaning up Downloaded Zips..."
          rm -rf External_source/dailymed/downloaded/*.zip
          
          echo "ðŸ’¾ Caching Database..."
          CACHE_DIR="production_data/dailymed_cache"
          mkdir -p "$CACHE_DIR"
          cp "production_data/dailymed_full_database.jsonl.gz" "$CACHE_DIR/"

      - name: 2.4 Commit DailyMed Database (If Small Enough)
        if: steps.check_dm_cache.outputs.skip_dm != 'true'
        run: |
           FILE="production_data/dailymed_full_database.jsonl.gz"
           SIZE=$(stat -c%s "$FILE")
           # Limit: 95MB (approx 99614720 bytes) to be safe for GitHub 100MB limit
           LIMIT=99000000
           
           if [ "$SIZE" -lt "$LIMIT" ]; then
               echo "âœ… File size ($SIZE) is within GitHub limits. Committing..."
               git add -f "$FILE"
               git commit -m "data: dailymed full db update $(date +'%Y-%m-%d')" || echo "No changes"
               git push --force-with-lease || git push
           else
               echo "âš ï¸ File size ($SIZE) exceeds 99MB safety limit. SKIPPING COMMIT to avoid rejection."
               echo "   The file is still cached locally in production_data/dailymed_cache/"
           fi
          
      # OPTIONAL: Commit a small sentinel file or strict data, NOT the 1GB file
      
      # --- STEP 3: Dosages (Extract -> Cache -> Clean) ---
      - name: 3.1 Check Dosages Cache
        id: check_dosages_cache
        run: |
          CACHE_DIR="production_data/final_cache"
          DOSAGES_CACHE="$CACHE_DIR/production_dosages.jsonl"
          TARGET_DOSAGES="production_data/production_dosages.jsonl"
          
          skip_dosages=false
          
          if [ -f "$DOSAGES_CACHE" ]; then
             echo "âœ… Found valid cached Dosages"
             cp "$DOSAGES_CACHE" "$TARGET_DOSAGES"
             skip_dosages=true
          fi
          echo "skip_dosages=$skip_dosages" >> $GITHUB_OUTPUT

      - name: 3.2 Extract Dosages
        if: steps.check_dosages_cache.outputs.skip_dosages != 'true'
        run: |
          # Reads production_data/dailymed_full_database.jsonl.gz
          python3 scripts/process_datalake.py
          
          # Save to Cache
          CACHE_DIR="production_data/final_cache"
          mkdir -p "$CACHE_DIR"
          cp "production_data/production_dosages.jsonl" "$CACHE_DIR/"
          echo "ðŸ’¾ Dosages cached"
          
      - name: 3.3 Commit Dosages Checkpoint
        if: steps.check_dosages_cache.outputs.skip_dosages != 'true'
        run: |
          git add -f production_data/production_dosages.jsonl
          git commit -m "data: dosages checkpoint $(date +'%Y-%m-%d')" || echo "No changes"
          git pull --rebase -X theirs
          git push

      # --- STEP 4: Interactions (Assuming it uses Zips? Re-download or skip?) ---
      # FIXME: Interaciton extractor usually needs Zips. If we deleted them in Step 2.3, this will fail.
      # SOLUTION: Move Zip Cleanup to AFTER Interactions. 
      # BUT: We ran out of space.
      # OPTIMIZATION: Interaction Extractor should use the JSONL Database if possible?
      # For now, let's DISABLE interaction extraction if zips are gone, OR move zip cleanup later.
      # To match user request "Extract Dosages -> Commit", then "Extract Interactions -> Commit".
      # We must move Zip Cleanup effectively.
      # Or, rely on the fact that we might skip downloading if cache exists.
      
      # Let's assume for this fix: We keep Zips until Interaction is done, BUT we trust GZIP saved enough space.
      # If space is critical, we should extract interactions BEFORE dosages or in parallel?
      
      # Let's clean Zips ONLY after Interactions.
      # BUT user reported space error.
      # Strategy: Extract Interactions FIRST (from zips) then delete zips, THEN extract dosages (from JSONL).
      # Interactions script (custom) reads Zips. Full DB extractor reads zips.
      # Both read zips.
      
      # Re-ordering:
      # 1. Scrape
      # 2. Download Zips
      # 3. Extract Full DB (JSONL.GZ)
      # 4. Extract Interactions (JSON) - reads Zips
      # 5. DELETE ZIPS (Huge space win)
      # 6. Extract Dosages (reads JSONL.GZ)
      # 7. Sync D1
      
      - name: 3.4 Check Interactions Cache
        id: check_interactions_cache
        run: |
          CACHE_DIR="production_data/final_cache"
          INTERACTIONS_CACHE="$CACHE_DIR/dailymed_interactions.json"
          TARGET_INTERACTIONS="production_data/dailymed_interactions.json"
          
          skip_interactions=false
          if [ -f "$INTERACTIONS_CACHE" ]; then
             echo "âœ… Found valid cached Interactions"
             cp "$INTERACTIONS_CACHE" "$TARGET_INTERACTIONS"
             skip_interactions=true
          fi
          echo "skip_interactions=$skip_interactions" >> $GITHUB_OUTPUT

      - name: 3.5 Extract Interactions
        if: steps.check_interactions_cache.outputs.skip_interactions != 'true' && steps.check_dm_cache.outputs.skip_dm != 'true'
        # Only run if we downloaded fresh data (zips exist)
        run: |
          # This script reads Zips from External_source/...
          python3 production_data/extract_dailymed_interactions.py
          
          CACHE_DIR="production_data/final_cache"
          mkdir -p "$CACHE_DIR"
          cp "production_data/dailymed_interactions.json" "$CACHE_DIR/"
          echo "ðŸ’¾ Interactions cached"

      - name: 3.6 Commit Interactions Checkpoint
        if: steps.check_interactions_cache.outputs.skip_interactions != 'true'
        run: |
          git add -f production_data/dailymed_interactions.json
          git commit -m "data: interactions checkpoint $(date +'%Y-%m-%d')" || echo "No changes"
          git pull --rebase -X theirs
          git push

      - name: 3.7 FINAL CLEANUP (Crucial for Space)
        if: always()
        run: |
          echo "ðŸ§¹ Cleaning up Zips and Large Temp Files..."
          rm -rf External_source/dailymed/downloaded/*.zip
          # We keep the JSONL.GZ as it is the data lake for dosages (used in step 3.2 which is now re-ordered in my head, but strictly speaking steps are sequential)
          # Wait, I put Extract Dosages (3.2) BEFORE this. So zips are safe to delete now.

      - name: 3.8 Merge Hybrid Data
        run: |
          python3 scripts/merge_hybrid_data.py

      # --- STEP 5: Asset Generation & D1 Sync ---
      - name: 4. Bootstrap App Assets
        run: |
          python3 scripts/bootstrap_app_data.py
          
      - name: 4.5 Commit Final DB
        run: |
          git add -f assets/data/dosage_guidelines.json
          git add -f assets/data/drug_interactions.json
          git add -f assets/meds.csv
          
          git commit -m "db: full rebuild complete $(date +'%Y-%m-%d')" || echo "No changes"
          git pull --rebase -X theirs
          git push

      - name: 5. Sync Full Database to D1
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          D1_DATABASE_ID: ${{ secrets.D1_DATABASE_ID }}
        run: |
          echo "::group::Syncing Drugs Table"
          python3 scripts/export_to_d1.py assets/meds.csv d1_import.sql
          
          npm install -g wrangler
          cd cloudflare-worker
          npx wrangler d1 execute mediswitch-db --file=../d1_import.sql --remote
          echo "::endgroup::"
          
          echo "::group::Syncing Dosages (Full)"
          # Assuming upload script exists
          if [ -f "../scripts/upload_dosage_d1.py" ]; then
            python3 ../scripts/upload_dosage_d1.py \
              --json-file ../assets/data/dosage_guidelines.json \
              --account-id "$CLOUDFLARE_ACCOUNT_ID" \
              --database-id "$D1_DATABASE_ID" \
              --api-token "$CLOUDFLARE_API_TOKEN"
          fi
          echo "::endgroup::"
          
          echo "::group::Syncing Interactions (Full)"
          if [ -f "../scripts/upload_interactions_d1.py" ]; then
            python3 ../scripts/upload_interactions_d1.py \
              --json-file ../assets/data/drug_interactions.json \
              --account-id "$CLOUDFLARE_ACCOUNT_ID" \
              --database-id "$D1_DATABASE_ID" \
              --api-token "$CLOUDFLARE_API_TOKEN" \
              --clear-first
          fi
          echo "::endgroup::"

      - name: Final Notification
        uses: rtCamp/action-slack-notify@v2
        if: always()
        env:
            SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK_URL }}
            SLACK_MESSAGE: 'Database Rebuild Job Finished (Status: ${{ job.status }})'
