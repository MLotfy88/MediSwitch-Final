#!/usr/bin/env python3
"""
DDInter2 Ultimate Scraper v10 - API Edition
============================================
Ø³ÙƒØ±Ø§Ø¨Ø± Ù†Ù‡Ø§Ø¦ÙŠ ÙŠØ³ØªØ®Ø¯Ù… API endpoints Ø§Ù„Ù…ÙƒØªØ´ÙØ©
- Ø³Ø±Ø¹Ø© ÙØ§Ø¦Ù‚Ø© (100x Ø£Ø³Ø±Ø¹ Ù…Ù† Selenium)
- Ø¯Ø¹Ù… Resume/Ø§Ø³ØªÙƒÙ…Ø§Ù„ ÙƒØ§Ù…Ù„
- ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ GitHub Actions
- Ø¬Ù…Ø¹ Ø´Ø§Ù…Ù„ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

API Endpoints Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:
- /server/interact-with/{drug_id}/       â†’ Drug-Drug interactions
- /server/interact-with-food/{drug_id}/  â†’ Drug-Food interactions  
- /server/interact-with-multi/{drug_id}/ â†’ Compound preparations
"""

import re
import requests
import sqlite3
import json
import os
import time
import urllib3
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from collections import Counter
from bs4 import BeautifulSoup
import threading

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ============================================
# Configuration
# ============================================
DB_PATH = 'ddinter_complete.db'
SCHEMA_SQL = 'database_schema.sql'
DRUG_IDS_FILE = 'unique_drugs.json'
BASE_URL = 'https://ddinter2.scbdd.com'
MAX_WORKERS = 20
REQUEST_TIMEOUT = 30

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Referer': 'https://ddinter2.scbdd.com/',
    'Connection': 'keep-alive'
}

# ============================================
# Utility Classes
# ============================================
class ThreadSafeCounter:
    def __init__(self):
        self._value = 0
        self._lock = threading.Lock()

    def increment(self):
        with self._lock:
            self._value += 1
            return self._value

    def get(self):
        with self._lock:
            return self._value

stats = {
    'drugs_processed': ThreadSafeCounter(),
    'ddi_fetched': ThreadSafeCounter(),
    'dfi_fetched': ThreadSafeCounter(),
    'dsi_fetched': ThreadSafeCounter(),
    'multi_fetched': ThreadSafeCounter(),
    'errors': ThreadSafeCounter(),
    'details_enriched': ThreadSafeCounter()
}

# ============================================
# Phase 2: Detail Enrichment Functions
# ============================================
def fetch_interaction_details(interaction_id):
    """Ø¬Ù„Ø¨ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ù†ØµÙŠØ© Ù…Ù† ØµÙØ­Ø© HTML"""
    url = f"{BASE_URL}/server/interact/{interaction_id}/"
    try:
        response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. Interaction Description
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø®Ù„ÙŠØ© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ "Interaction" Ø«Ù… Ø§Ù„Ø®Ù„ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ„ÙŠÙ‡Ø§
        desc_cell = soup.find('td', class_='key', string=re.compile(r'Interaction', re.I))
        description = None
        if desc_cell:
            val_cell = desc_cell.find_next_sibling('td', class_='value')
            if val_cell:
                description = val_cell.get_text(strip=True)

        # 2. Management
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø®Ù„ÙŠØ© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ "Management"
        mgmt_cell = soup.find('td', class_='key', string=re.compile(r'Management', re.I))
        management = None
        if mgmt_cell:
            val_cell = mgmt_cell.find_next_sibling('td', class_='value')
            if val_cell:
                management = val_cell.get_text(strip=True)
                
        # 3. References
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù†ØµØ± Ø¨Ù…Ø¹Ø±Ù reference-text
        ref_elem = soup.find(id='reference-text')
        references = None
        if ref_elem:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø¯Ø§Ø®Ù„ span
            refs = [span.get_text(strip=True) for span in ref_elem.find_all('span')]
            if refs:
                references = "\\n".join(refs)
            else:
                references = ref_elem.get_text(strip=True)

        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ù†Ø¹ØªØ¨Ø±Ù‡Ø§ ÙØ´Ù„
        if not description and not management:
            return None

        return {
            'interaction_id': interaction_id,
            'interaction_description': description,
            'management_text': management,
            'reference_text': references
        }

    except Exception as e:
        # print(f"âš ï¸ Error details for {interaction_id}: {e}") # Silent error to reduce noise
        return None

def fetch_interaction_alternatives(interaction_id):
    """Ø¬Ù„Ø¨ Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ Ø§Ù„Ø¢Ù…Ù†Ø© Ù…Ù† ØµÙØ­Ø© Ø§Ù„ØªÙØ§Ø¹Ù„"""
    url = f"{BASE_URL}/server/interact/{interaction_id}/"
    try:
        response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
        if response.status_code != 200:
            return None, None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„
        alternatives_a = []
        alternatives_b = []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù‚Ø³Ù… "Alternative for"
        key_cells = soup.find_all('td', class_='key')
        for i, key_cell in enumerate(key_cells):
            text = key_cell.get_text(strip=True)
            if 'Alternative for' in text:
                value_cell = key_cell.find_next_sibling('td', class_='value')
                if value_cell:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¯ÙˆÙŠØ© Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
                    links = value_cell.find_all('a', href=lambda x: x and 'drug-detail' in x)
                    alts = [link.get_text(strip=True) for link in links if link.get_text(strip=True)]
                    
                    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯ÙˆØ§Ø¡ A Ø£Ùˆ B
                    if len(alternatives_a) == 0:
                        alternatives_a = alts
                    else:
                        alternatives_b = alts
        
        return alternatives_a if alternatives_a else None, alternatives_b if alternatives_b else None
        
    except Exception as e:
        # print(f"âš ï¸ Error fetching alternatives for {interaction_id}: {e}")
        return None, None


def update_interaction_details(details):
    """ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ alternatives"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    try:
        c.execute('''
            UPDATE drug_drug_interactions
            SET interaction_description = ?,
                management_text = ?,
                reference_text = ?,
                alternative_drugs_a = ?,
                alternative_drugs_b = ?
            WHERE interaction_id = ?
        ''', (
            details.get('interaction_description'),
            details.get('management_text'),
            details.get('reference_text'),
            details.get('alternatives_a'),
            details.get('alternatives_b'),
            details.get('interaction_id')
        ))
        conn.commit()
        return True
    except Exception as e:
        print(f"âŒ DB Error update {details['interaction_id']}: {e}")
        return False
    finally:
        conn.close()

def process_enrichment_item(interaction_id):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†ØµØ± ÙˆØ§Ø­Ø¯ ÙÙŠ Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø¥Ø«Ø±Ø§Ø¡ - Ù…Ø¹ alternatives"""
    details = fetch_interaction_details(interaction_id)
    if details:
        # Ø¬Ù„Ø¨ Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ Ø£ÙŠØ¶Ø§Ù‹
        alt_a, alt_b = fetch_interaction_alternatives(interaction_id)
        if alt_a or alt_b:
            details['alternatives_a'] = json.dumps(alt_a) if alt_a else None
            details['alternatives_b'] = json.dumps(alt_b) if alt_b else None
        
        if update_interaction_details(details):
            stats['details_enriched'].increment()
            return True
    return False

def get_interactions_needing_enrichment():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙ†Ù‚ØµÙ‡Ø§ Ø§Ù„ØªÙØ§ØµÙŠÙ„"""
    print("running query to find missing details...") 
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    # Ù†Ø®ØªØ§Ø± Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„ØªÙŠ ÙÙŠÙ‡Ø§ Ø§Ù„ÙˆØµÙ ÙØ§Ø±Øº
    c.execute("SELECT interaction_id FROM drug_drug_interactions WHERE interaction_description IS NULL OR interaction_description = ''")
    ids = [row[0] for row in c.fetchall()]
    conn.close()
    return ids

def run_phase_2_enrichment():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„ØªÙØ§ØµÙŠÙ„"""
    print("\n" + "="*70)
    print("ğŸš€ Phase 2: Enriching Interaction Details (Texts)")
    print("="*70)
    
    missing_ids = get_interactions_needing_enrichment()
    
    if not missing_ids:
        print("âœ… No interactions pending enrichment! (All have descriptions)")
        return

    print(f"ğŸ“¦ Found {len(missing_ids)} interactions needing text details.")
    print(f"ğŸ”„ Starting enrichment with {MAX_WORKERS} workers...")
    
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¹Ù…Ù„ Ø¥Ù„Ù‰ Ø¯ÙØ¹Ø§Øª ØµØºÙŠØ±Ø© Ù„ØªØ­Ø¯ÙŠØ« ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        batch_size = 1000
        total_processed = 0
        
        for i in range(0, len(missing_ids), batch_size):
            batch = missing_ids[i:i+batch_size]
            futures = {executor.submit(process_enrichment_item, iid): iid for iid in batch}
            
            for future in as_completed(futures):
                iid = futures[future]
                try:
                    future.result()
                except Exception:
                    pass
            
            total_processed += len(batch)
            elapsed = time.time() - start_time
            rate = total_processed / elapsed if elapsed > 0 else 0
            remaining = len(missing_ids) - total_processed
            eta = remaining / rate / 60 if rate > 0 else 0
            
            print(f"ğŸ“ˆ Progress: {stats['details_enriched'].get()}/{len(missing_ids)} | Rate: {rate:.1f}/s | ETA: {eta:.1f} min")

    print("âœ… Phase 2 Completed!")


# ============================================
# Main Execution
# ============================================
def main():
    print("="*70)
    print("ğŸš€ DDInter2 Ultimate Scraper v10.1 - Full Stack")
    print("="*70)
    print(f"â° Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    if not os.path.exists(DB_PATH):
        if not init_database():
            return
    else:
        print(f"ğŸ“¦ Using existing database: {DB_PATH}")
    
    # Phase 1: API Scraping (IDs & Lists)
    print("\nğŸ”¹ Checking Phase 1 (Core Data)...")
    all_drug_ids = load_drug_ids()
    if all_drug_ids:
        pending_drugs = get_pending_drugs(all_drug_ids)
        if pending_drugs:
            print(f"\nğŸ”„ Phase 1: Processing {len(pending_drugs)} remaining drugs...")
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = {executor.submit(process_single_drug, drug_id): drug_id for drug_id in pending_drugs}
                for future in as_completed(futures):
                    try:
                        future.result()
                    except:
                        pass
        else:
            print("âœ… Phase 1 Complete (All drugs processed).")

    # Phase 2: Detail Enrichment
    print("\nğŸ”¹ Checking Phase 2 (Text Details)...")
    run_phase_2_enrichment()
    
    # Final Stats
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT COUNT(*) FROM drugs")
    total_drugs = c.fetchone()[0]
    c.execute("SELECT COUNT(*) FROM drug_drug_interactions")
    total_ddi = c.fetchone()[0]
    c.execute("SELECT COUNT(*) FROM drug_drug_interactions WHERE interaction_description IS NOT NULL AND interaction_description != ''")
    enriched_ddi = c.fetchone()[0]
    conn.close()
    
    print(f"\nğŸ“Š Final Statistics:")
    print(f"   Drugs: {total_drugs}")
    print(f"   Interactions: {total_ddi}")
    print(f"   Enriched with Text: {enriched_ddi} ({(enriched_ddi/total_ddi*100) if total_ddi else 0:.1f}%)")
    print("="*70)

# ============================================
# Database Functions
# ============================================
def init_database():
    """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    print("ğŸ“¦ Initializing database...")
    
    if not os.path.exists(SCHEMA_SQL):
        print(f"âŒ Schema file not found: {SCHEMA_SQL}")
        return False
    
    conn = sqlite3.connect(DB_PATH)
    with open(SCHEMA_SQL, 'r', encoding='utf-8') as f:
        conn.executescript(f.read())
    conn.commit()
    conn.close()
    
    print(f"âœ… Database initialized: {DB_PATH}")
    return True

def load_drug_ids():
    """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ø£Ø¯ÙˆÙŠØ©"""
    if not os.path.exists(DRUG_IDS_FILE):
        print(f"âŒ Drug IDs file not found: {DRUG_IDS_FILE}")
        return []
    
    with open(DRUG_IDS_FILE, 'r') as f:
        data = json.load(f)
        drug_ids = data.get('unique_drugs', [])
        print(f"ğŸ“‹ Loaded {len(drug_ids)} drug IDs")
        return drug_ids

def mark_drug_processed(drug_id, status='completed', error_msg=None):
    """ØªØ³Ø¬ÙŠÙ„ Ø­Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙˆØ§Ø¡"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    try:
        c.execute('''
            INSERT OR REPLACE INTO scraping_progress (entity_type, entity_id, status, error_message)
            VALUES ('drug', ?, ?, ?)
        ''', (drug_id, status, error_msg))
        conn.commit()
    finally:
        conn.close()

def get_pending_drugs(all_drug_ids):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø¯ÙˆÙŠØ© Ø§Ù„ØªÙŠ Ù„Ù… ØªØªÙ… Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT entity_id FROM scraping_progress WHERE entity_type='drug' AND status='completed'")
    processed = set(row[0] for row in c.fetchall())
    conn.close()
    
    pending = [drug_id for drug_id in all_drug_ids if drug_id not in processed]
    print(f"ğŸ“Š Status: {len(processed)} completed, {len(pending)} pending")
    return pending

# ============================================
# HTML Scraping (Basic Info)
# ============================================
def extract_table_value(soup, key_text):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ÙŠÙ…Ø© Ù…Ù† Ø¬Ø¯ÙˆÙ„ HTML"""
    try:
        import re
        key_td = soup.find('td', class_='key', string=re.compile(key_text, re.I))
        if key_td:
            value_td = key_td.find_next_sibling('td', class_='value')
            if value_td:
                return value_td.get_text(strip=True)
    except:
        pass
    return None

def extract_drug_basic_info(drug_id):
    """Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø¯ÙˆØ§Ø¡ Ù…Ù† ØµÙØ­Ø© drug-detail"""
    url = f"{BASE_URL}/server/drug-detail/{drug_id}/"
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆØ§Ø¡ Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        drug_name = None
        title_elem = soup.find('strong', string=re.compile('Drugs Information:', re.I))
        if title_elem and title_elem.next_sibling:
            drug_name = title_elem.next_sibling.strip()
        
        # 2D SVG
        svg_elem = soup.find('svg')
        svg_content = str(svg_elem) if svg_elem else None

        # ATC Codes
        atc_td = soup.find('td', class_='key', string=re.compile('ATC Classification', re.I))
        atc_codes = []
        if atc_td:
            val_td = atc_td.find_next_sibling('td', class_='value')
            if val_td:
                atc_codes = [span.get_text(strip=True) for span in val_td.find_all('span', class_='badge')]

        # External Links
        links_td = soup.find('td', class_='key', string=re.compile('Useful Links', re.I))
        external_links = {}
        if links_td:
            val_td = links_td.find_next_sibling('td', class_='value')
            if val_td:
                for a in val_td.find_all('a', href=True):
                    site_name = a.get_text(strip=True)
                    url = a['href']
                    if site_name:
                        external_links[site_name] = url

        drug_data = {
            'ddinter_id': drug_id,
            'drug_name': drug_name,
            'drug_type': extract_table_value(soup, 'Drug Type'),
            'molecular_formula': extract_table_value(soup, 'Molecular Formula'),
            'molecular_weight': extract_table_value(soup, 'Molecular Weight'),
            'cas_number': extract_table_value(soup, 'CAS Number'),
            'description': extract_table_value(soup, 'Description'),
            'iupac_name': extract_table_value(soup, 'IUPAC Name'),
            'inchi': extract_table_value(soup, 'InChI'),
            'smiles': extract_table_value(soup, 'Canonical SMILES'),
            'atc_codes': json.dumps(atc_codes),
            'external_links': json.dumps(external_links),
            'structure_2d_svg': svg_content
        } 
        
        return drug_data
        
    except Exception as e:
        print(f"âš ï¸ Error fetching basic info for {drug_id}: {e}")
        return None

# ============================================
# API Calls (Interactions)
# ============================================
def fetch_drug_drug_interactions(drug_id):
    """Ø¬Ù„Ø¨ ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-Ø¯ÙˆØ§Ø¡ Ø¹Ø¨Ø± API Ù…Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ mechanisms"""
    url = f"{BASE_URL}/server/interact-with/{drug_id}/"
    interactions = []
    
    try:
        # Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„ÙŠ
        data = {
            'draw': 1,
            'start': 0,
            'length': 100,  # Ø¬Ù„Ø¨ 100 ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©
            'severity': '',
            'mechanism': ''
        }
        
        response = requests.post(url, data=data, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
        if response.status_code != 200:
            return []
        
        json_response = response.json()
        total_records = json_response.get('recordsTotal', 0)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ mechanisms
        for item in json_response.get('data', []):
            mechanisms = []
            if str(item.get('metabolism', '0')) == '1':
                mechanisms.append('Metabolism')
            if str(item.get('synergistic_effect', '0')) == '1':
                mechanisms.append('Synergism')
            if str(item.get('antagonistic_effect', '0')) == '1':
                mechanisms.append('Antagonism')
            if str(item.get('absorption', '0')) == '1':
                mechanisms.append('Absorption')
            if str(item.get('distribution', '0')) == '1':
                mechanisms.append('Distribution')
            if str(item.get('excretion', '0')) == '1':
                mechanisms.append('Excretion')
            if str(item.get('others', '0')) == '1':
                mechanisms.append('Others')
            
            interactions.append({
                'interaction_id': item.get('interaction_id'),
                'drug_id': item.get('drug_id'),
                'level': item.get('level'),
                'mechanisms': json.dumps(mechanisms) if mechanisms else None
            })
        
        # Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©
        for offset in range(100, total_records, 100):
            data['start'] = offset
            data['draw'] += 1
            
            response = requests.post(url, data=data, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
            if response.status_code == 200:
                json_response = response.json()
                for item in json_response.get('data', []):
                    mechanisms = []
                    if str(item.get('metabolism', '0')) == '1':
                        mechanisms.append('Metabolism')
                    if str(item.get('synergistic_effect', '0')) == '1':
                        mechanisms.append('Synergism')
                    if str(item.get('antagonistic_effect', '0')) == '1':
                        mechanisms.append('Antagonism')
                    if str(item.get('absorption', '0')) == '1':
                        mechanisms.append('Absorption')
                    if str(item.get('distribution', '0')) == '1':
                        mechanisms.append('Distribution')
                    if str(item.get('excretion', '0')) == '1':
                        mechanisms.append('Excretion')
                    if str(item.get('others', '0')) == '1':
                        mechanisms.append('Others')
                    
                    interactions.append({
                        'interaction_id': item.get('interaction_id'),
                        'drug_id': item.get('drug_id'),
                        'level': item.get('level'),
                        'mechanisms': json.dumps(mechanisms) if mechanisms else None
                    })
                
        stats['ddi_fetched'].increment()
        return interactions
        
    except Exception as e:
        print(f"âš ï¸ Error fetching DDI for {drug_id}: {e}")
        return []

def fetch_drug_food_interactions(drug_id):
    """Ø¬Ù„Ø¨ ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-ØºØ°Ø§Ø¡ Ø¹Ø¨Ø± API"""
    url = f"{BASE_URL}/server/interact-with-food/{drug_id}/"
    interactions = []
    
    try:
        data = {
            'draw': 1,
            'start': 0,
            'length': 100,
            'severity': '',
            'mechanism': ''
        }
        
        response = requests.post(url, data=data, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
        if response.status_code != 200:
            return []
        
        json_response = response.json()
        total_records = json_response.get('recordsTotal', 0)
        interactions.extend(json_response.get('data', []))
        
        for offset in range(100, total_records, 100):
            data['start'] = offset
            data['draw'] += 1
            
            response = requests.post(url, data=data, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
            if response.status_code == 200:
                json_response = response.json()
                interactions.extend(json_response.get('data', []))
        
        stats['dfi_fetched'].increment()
        return interactions
        
    except Exception as e:
        print(f"âš ï¸ Error fetching DFI for {drug_id}: {e}")
        return []

def fetch_compound_preparations(drug_id):
    """Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ­Ø¶Ø±Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ¨Ø© Ø¹Ø¨Ø± API"""
    url = f"{BASE_URL}/server/interact-with-multi/{drug_id}/"
    preparations = []
    
    try:
        data = {
            'draw': 1,
            'start': 0,
            'length': 100
        }
        
        response = requests.post(url, data=data, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
        if response.status_code != 200:
            return []
        
        json_response = response.json()
        total_records = json_response.get('recordsTotal', 0)
        preparations.extend(json_response.get('data', []))
        
        for offset in range(100, total_records, 100):
            data['start'] = offset
            data['draw'] += 1
            
            response = requests.post(url, data=data, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
            if response.status_code == 200:
                json_response = response.json()
                preparations.extend(json_response.get('data', []))
        
        stats['multi_fetched'].increment()
        return preparations
        
    except Exception as e:
        print(f"âš ï¸ Error fetching preparations for {drug_id}: {e}")
        return []

def fetch_drug_disease_interactions(drug_id):
    """Ø¬Ù„Ø¨ ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-Ù…Ø±Ø¶ Ø¹Ø¨Ø± API"""
    url = f"{BASE_URL}/server/interact-with-dis/{drug_id}/"
    interactions = []
    
    try:
        data = {
            'draw': 1,
            'start': 0,
            'length': 100,
            'severity': ''
        }
        
        response = requests.post(url, data=data, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
        if response.status_code != 200:
            return []
        
        json_response = response.json()
        total_records = json_response.get('recordsTotal', 0)
        interactions.extend(json_response.get('data', []))
        
        for offset in range(100, total_records, 100):
            data['start'] = offset
            data['draw'] += 1
            
            response = requests.post(url, data=data, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)
            if response.status_code == 200:
                json_response = response.json()
                interactions.extend(json_response.get('data', []))
        
        stats['dsi_fetched'].increment()
        return interactions
        
    except Exception as e:
        print(f"âš ï¸ Error fetching DSI for {drug_id}: {e}")
        return []

# ============================================
# Database Saving
# ============================================
def save_drug_data(drug_data, ddi_list, dfi_list, prep_list, dsi_list):
    """Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ø¡ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    
    try:
        # 1. Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ø¡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        c.execute('''
            INSERT OR REPLACE INTO drugs 
            (ddinter_id, drug_name, drug_type, molecular_formula, molecular_weight, 
             cas_number, description, iupac_name, inchi, smiles, atc_codes, external_links, structure_2d_svg)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            drug_data['ddinter_id'], drug_data['drug_name'], drug_data['drug_type'],
            drug_data['molecular_formula'], drug_data['molecular_weight'],
            drug_data['cas_number'], drug_data['description'], drug_data['iupac_name'],
            drug_data['inchi'], drug_data['smiles'], drug_data['atc_codes'],
            drug_data['external_links'], drug_data['structure_2d_svg']
        ))
        
        # 2. Ø­ÙØ¸ ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-Ø¯ÙˆØ§Ø¡ Ù…Ø¹ mechanism_flags
        for interaction in ddi_list:
            c.execute('''
                INSERT OR IGNORE INTO drug_drug_interactions 
                (interaction_id, drug_a_id, drug_b_id, severity, mechanism_flags, source_url)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                interaction.get('interaction_id'),
                drug_data['ddinter_id'],
                interaction.get('drug_id'),
                {1: 'Minor', 2: 'Moderate', 3: 'Major'}.get(interaction.get('level'), 'Unknown'),
                interaction.get('mechanisms'),
                f"{BASE_URL}/server/interact/{interaction.get('interaction_id')}/"
            ))
        
        # 3. Ø­ÙØ¸ ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-ØºØ°Ø§Ø¡
        for interaction in dfi_list:
            c.execute('''
                INSERT OR IGNORE INTO drug_food_interactions 
                (drug_id, food_name, severity, description, management, mechanism_flags)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                drug_data['ddinter_id'],
                interaction.get('foodName'),
                {1: 'Minor', 2: 'Moderate', 3: 'Major'}.get(int(interaction.get('level', 0)), 'Unknown'),
                interaction.get('newInteraction'),
                interaction.get('newManagement'),
                interaction.get('magnesium')
            ))
        
        # 4. Ø­ÙØ¸ Ø§Ù„Ù…Ø³ØªØ­Ø¶Ø±Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ¨Ø©
        for prep in prep_list:
            c.execute('''
                INSERT OR IGNORE INTO compound_preparations 
                (drug_id, preparation_name, components, interaction_info)
                VALUES (?, ?, ?, ?)
            ''', (
                drug_data['ddinter_id'],
                prep.get('trade_name'),
                json.dumps(prep.get('multi_drug', [])),
                prep.get('warning')
            ))
        
        # 5. Ø­ÙØ¸ ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ø£Ù…Ø±Ø§Ø¶
        for dsi in dsi_list:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ ÙƒÙ‚Ø§Ø¦Ù…Ø© JSON Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
            refs = dsi.get('references', [])
            if isinstance(refs, str):
                refs = refs.split('|')
            
            c.execute('''
                INSERT OR IGNORE INTO drug_disease_interactions 
                (drug_id, disease_name, severity, interaction_text, reference_text)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                drug_data['ddinter_id'],
                dsi.get('diseaseName'),
                {1: 'Minor', 2: 'Moderate', 3: 'Major'}.get(int(dsi.get('level', 0)), 'Unknown'),
                dsi.get('text'),
                json.dumps(refs)
            ))
        
        conn.commit()
        return True
        
    except Exception as e:
        conn.rollback()
        print(f"âŒ Error saving {drug_data['ddinter_id']}: {e}")
        return False
    finally:
        conn.close()

# ============================================
# Main Processing
# ============================================
def process_single_drug(drug_id):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙˆØ§Ø¡ ÙˆØ§Ø­Ø¯ - Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§ØªÙ‡"""
    try:
        # 1. Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        drug_data = extract_drug_basic_info(drug_id)
        if not drug_data:
            mark_drug_processed(drug_id, 'failed', 'Failed to fetch basic info')
            stats['errors'].increment()
            return False
        
        # 2. ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-Ø¯ÙˆØ§Ø¡
        ddi_list = fetch_drug_drug_interactions(drug_id)
        
        # 3. ØªÙØ§Ø¹Ù„Ø§Øª Ø¯ÙˆØ§Ø¡-ØºØ°Ø§Ø¡
        dfi_list = fetch_drug_food_interactions(drug_id)
        
        # 4. Ø§Ù„Ù…Ø³ØªØ­Ø¶Ø±Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ¨Ø©
        prep_list = fetch_compound_preparations(drug_id)

        # 5. ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ø£Ù…Ø±Ø§Ø¶
        dsi_list = fetch_drug_disease_interactions(drug_id)
        
        # 6. Ø­ÙØ¸ ÙƒÙ„ Ø´ÙŠØ¡
        if save_drug_data(drug_data, ddi_list, dfi_list, prep_list, dsi_list):
            mark_drug_processed(drug_id, 'completed')
            
            count = stats['drugs_processed'].increment()
            if count % 10 == 0:
                print(f"âœ… Progress: {count} drugs | DDI: {stats['ddi_fetched'].get()} | DFI: {stats['dfi_fetched'].get()} | Multi: {stats['multi_fetched'].get()} | Errors: {stats['errors'].get()}")
            
            return True
        else:
            mark_drug_processed(drug_id, 'failed', 'Database save failed')
            stats['errors'].increment()
            return False
            
    except Exception as e:
        mark_drug_processed(drug_id, 'failed', str(e))
        stats['errors'].increment()
        print(f"âŒ Error processing {drug_id}: {e}")
        return False



if __name__ == "__main__":
    main()
